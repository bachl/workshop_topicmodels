[
["index.html", "Mini-Workshop (Structural) Topic Models Abschnitt 1 Überblick 1.1 Inhalt des virtuellen Mini-Workshops 1.2 Welche Inhalte wir nicht behandeln 1.3 Aufbau des Workshops", " Mini-Workshop (Structural) Topic Models Marko Bachl Sommersemester 2020 | IJK Hannover Abschnitt 1 Überblick 1.1 Inhalt des virtuellen Mini-Workshops In diesem Mini-Workshop erläutere ich das praktische Vorgehen einer Datenanalyse mit Structural Topic Models. Wir behandeln die folgenden Schritte im Analyseprozess: Modellspezifikation Modellvergleich zur Auswahl eines geeigneten Modells Darstellung der Ergebnisse Weitere Analysen Zusammenhänge der Topic-Prävalenz mit Kovariaten Identifikation verwandter Topics Wir verwenden das Paket {stm} (Roberts, Stewart, and Tingley 2019) zum Schätzen von Topic Models. Für die Variante der Structural Topic Models und die Implementation in diesem Paket sprechen für mich die folgenden Gründe Gute Integration mit R und Paketen, die ich für die Arbeit mit Text-Daten verwende (insbesondere {quanteda} und {tidytext}) Gute ergänzende Pakete zur Arbeit mit den Modellen (insbesondere {stminsights} und {oolong}) Vergleichsweise schnelle Modellschätzung auch mit großen Datensätzen Direktes Schätzen von Zusammenhängen von Topics mit Kovariaten Initialisieren der Modellschätzung mit dem Spectral Algorithmus Recht weit verbreitet in einem Feld, in dem ich viel lese (Politische Kommunkation nach einem weitem Verständnis) Die Darstellung basiert auf einer Analyse, die ich gemeinsam mit Elena Link durchgeführt habe. Wir untersuchten, wie das Thema Impfen in Online-Foren für Eltern diskutiert wurde. Wir verwenden aber nur einen nicht repräsentativen Ausschnitt aus dem Material, um die notwendige Rechenleistung und -zeit zu verringern. Einen Preprint zur Analyse könnt ihr hier lesen: Vaccine-related Discussions in Online Communities for Parents. A Quantitative Overview. Die Dokumentation zur Studie ist hier verfügbar: https://bachl.github.io/vaccine_discussions/. Daten und Analyse-Skripts gibt es im OSF. Dort werden auch die Datenerhebung mittels Web-Scraping und die Datenaufbereitung erläutert. Diese Inhalte sind nicht Teil dieses Workshops. Wenn ihr Fragen dazu habt, dürft ihr sie natürlich stellen. 1.2 Welche Inhalte wir nicht behandeln Auch wenn das im direkten Vergleich mit dem Parallel-Angebot zu Panel Data Analysis (meine Ausführlichkeit dort sind ein Grund für die spätere Lieferung dieser Materialien) enttäuschend sein mag: Die Inhalte in diesem Mini-Workshop entsprechen in ihrem Umfang wirklich nur dem, was ich zu Beginn des Digital-Semesters geplant und angekündigt hatte. Der Mini-Workshop ersetzt keine tiefer gehende Einarbeitung in die Methode, sondern ist als ein Einstieg zu verstehen. Wir behandeln hier keine theoretischen, statistischen oder auf die Software-Implementierung der Modellschätzung bezogenen Fragen. Die Grundlagen dazu können aus den Texten im LMS entnommen werden (Maier et al. 2018; Roberts, Stewart, and Tingley 2019). Es gibt neben {stm} viele andere Implementationen in R und ihn anderer Software. Gefühlt gibt es alle 6 Monate eine neue Variante von Topic Models, alle 3 Monate eine neue Implementierung und jeden Monat ein Paket mit zusätzlichen Tools für die Arbeit mit Topic Models. Meine Entscheidung für {stm} ist keine informierte Entscheidung gegen andere Varianten, Implementierungen und Tools. Dieser Workshop ist keine Aufforderung, ausschließlich {stm} zu nutzen. Informiert euch gegebenenfalls selbst über Software-Lösungen, die für eure Bedürfnisse geeignet sind. Dieser Mini-Workshop ist kein R-Tutorial. Wenn ihr Interesse habt, R-Kenntnisse zu erwerben und zu vertiefen, empfehle ich R4DS. Dieser Mini-Workshop ist keine allgemeine Einführung in die computergestützte Inhaltsanalyse. Wenn ihr allgemein Textanalysen mit R durchführen möchtet, empfehle ich zu diesem Thema die Einführung von Cornelius Puschmann. 1.3 Aufbau des Workshops Inhaltlicher Aufbau: Siehe Kapitel-Gliederung Material Dieses Dokument + R Skripte: (Hoffentlich) mehr oder weniger selbsterklärendes Material Daten: Ein Ausschnitt auf den Daten der oben genannten Beispielstudie. Eine genauere Beschreibung folgt im nächsten Abschnitt. Sie sind nicht auf GitHub, sondern nur im LMS verfügbar. Screencast: Zu einigen Analyseschritten stelle ich Screencasts zur Verfügung (siehe Anmerkungen im Text). Übungen: Zur Einübung der Inhalte empfehle ich, die in diesem Material dokumentierten Code-Auszüge auf dem eigenen Rechner auszuführen und anzupassen. Die inhaltliche Interpretation der Modelle wird im Material nur angedeutet. Ihr könnt diese selbst vertiefen. Pakete Wir verwenden die folgenden Pakete if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load(tidyverse, stm, stminsights, tidytext, quanteda, lubridate, knitr, tictoc, furrr, oolong, textmineR, ggdendro) theme_set(theme_bw()) # ggplot theme tibble(package = c(&quot;R&quot;, sort(pacman::p_loaded()))) %&gt;% mutate(version = map_chr(package, ~as.character(pacman::p_version(package = .x)))) %&gt;% knitr::kable() package version R 3.6.2 dplyr 1.0.0 forcats 0.4.0 furrr 0.1.0 future 1.16.0 ggdendro 0.1.20 ggplot2 3.3.2 knitr 1.28 lubridate 1.7.4 Matrix 1.2.18 oolong 0.3.8 pacman 0.5.1 purrr 0.3.4 quanteda 2.0.1 readr 1.3.1 stm 1.3.5 stminsights 0.4.0 stringr 1.4.0 textmineR 3.0.4 tibble 3.0.1 tictoc 1.0 tidyr 1.0.2 tidytext 0.2.3 tidyverse 1.3.0 Literatur "],
["beispiel-daten-und-aufbereitung.html", "Abschnitt 2 Beispiel-Daten und Aufbereitung 2.1 Laden der Daten und Übersicht 2.2 Aufbereitung für das Schätzen der Topic Models", " Abschnitt 2 Beispiel-Daten und Aufbereitung 2.1 Laden der Daten und Übersicht Wir verwenden einen Ausschnitt der Daten aus der Beispielstudie. Konkret handelt es sich um Posts mit dem Suchwort impf, die zwischen dem 1. Mai 2016 und dem 8. Juli 2019 im Elternforum Urbia veröffentlicht wurden. Ausgeschlossen wurden unter anderem sehr kurze Posts (weniger als 20 Wörter) Posts mit dem Wort schimpf Posts zur Impfung von Haustieren (nach einem kurzen Diktionär) Die Dokumentation zur Studie gibt weitere Informationen zur Erhebung und Bereinigung der Rohdaten. Diese Daten können aus Copyright- und Privacy-Gründen nicht auf GitHub veröffentlicht werden. Ich habe Sie daher im LMS hochgeladen. Bitte ladet die ZIP-Datei herunter. Wenn ihr sie mit dem Code aus dem Repository integrieren wollt, müsst ihr sie in den Ordner “data” unter “R” entpacken. # Laden der Daten d = read_rds(&quot;R/data/example_data.rds&quot;) d %&gt;% print(n = 5) ## # A tibble: 12,369 x 5 ## post author postdate wc thread_title ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;int&gt; &lt;chr&gt; ## 1 Wenn Impfungen zu Todesfäll… zwerg-b… 2018-04-06 26 HPV-Impfung ## 2 Hallo Moni Danke für deine … Inaktiv 2017-06-03 21 Warum so oft Scheidenp… ## 3 Hallo ja sind glaube ich dr… danerl 2017-06-05 42 Warum so oft Scheidenp… ## 4 Guten Morgen, gibt es hier … butterf… 2017-05-14 133 Impfung Deutschland/Ös… ## 5 In Österreich wird im 3., 5… butterf… 2017-05-15 68 Impfung Deutschland/Ös… ## # … with 12,364 more rows d %&gt;% mutate(ym = round_date(postdate, &quot;month&quot;)) %&gt;% count(ym) %&gt;% ggplot(aes(ym, n)) + geom_line() d %&gt;% pull(&quot;wc&quot;) %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 20 37 60 85 101 2493 Der Datensatz besteht aus 12,369 Posts. Die Variable post enthält den vollen Text des Posts. Die Variable author enthält den Accountnamen, von dem der Post abgegeben wurde. Die Variable date enthält den Tag der Veröffentlichung. Die Variable wc enthält die Zahl der Wörter des Posts. Die Variable thread_title enthält den Titel des Diskussions-Threads. Pro Monat sind zwischen ca. 120 und 1.000 Posts in unserer Stichprobe. Typische Posts haben einen Umfang von zwischen 40 und 100 Wörtern (Zur Erinnerung: Sehr kurze Post wurden bereits ausgeschlossen). 2.2 Aufbereitung für das Schätzen der Topic Models Grundsätzlich gilt: Die verschiedenen Schritte bei der Aufbereitung des Text-Korpus kann die Ergebnisse wesentlich beeinflussen (Denny and Spirling 2018; Maier et al. 2018). Aber ist es häufig sehr schwierig, theoretisch informierte Entscheidungen zu treffen, da unsere Theorien fast immer zu vage sind, um etwas über konkrete, manifeste Eigenschaften der Texte auszusagen es schwer ist, die Folge einer Entscheidung für das technische Schätzen der Modelle und für die substanzielle Interpretation der Ergebnisse vorherzusagen, Entscheidungen post hoc auf Basis der Ergebnisse wissenschaftstheoretisch und -praktisch problematisch sein können (overfitting, harking bzw. hindsight bias, etc.). In der zugrunde liegenden Studie habe ich versucht, diese Entscheidungen a priori zu treffen. Die Entscheidungen basieren aber zugegebenermaßen mehr auf vagen Vermutungen und für mich plausiblen und pragmatischen Überlegungen als auf einer konsistenten Theorie. Entfernen von Stoppwörtern: Stoppwörter sind Wörter, die in einer Sprache häufig vorkommen und nicht wesentlich zur Bedeutung eines Texts beitragen. Hier habe ich auf Basis der deutschen Liste im Paket {stopwords} und der Worthäufigkeiten im Korpus eine Liste erstellt. Durch das Pruning der Dokument-Feature-Matrix (siehe unten) ist die Auswahl der Stoppwörter aber weniger entscheidend, da Wörter, die in sehr vielen Texten des Korpus vorkommen, ohnehin entfernt werden. Zusätzliche Berücksichtigung von Bi- und Tri-Grammen: Ich habe die Kombinationen von zwei oder drei Wörtern, die häufig im Korpus vorkamen, daraufhin gesichtet, ob sie für das Thema Impfen und gesundheitsrelevante Diskussionen zusätzliche Informationen enthalten, die jedes einzelne Wort alleine nicht enthält. Diese Kombinationen wurden als zusätzliche Features aufgenommen. Der Argumentation und den empirischen Ergebnissen von Schofield and Mimno (2016) (deren Aufsatz übrigens einen großartigen Titel hat, großer NLP Nerd Humor) folgend habe ich auf Stemming oder Lemmatisierung verzichtet. In der Tat zeigt sich, dass Wörter mit dem gleichen Wortstamm, wie von Schofield and Mimno (2016) beschrieben, häufig im selben Topic landen. Üblichen Standards (z.B. Maier et al. 2018) folgend habe ich alle Wörter in Kleinschreibung umgewandelt, Satzzeichen entfernt und URL entfernt. Zahlen habe ich beibehalten, da sie (wie die Ergebnisse auch zeigen) typische Merkmale bestimmter Perspektiven auf das Thema Impfen sind. Da wir auch an der Veränderung der Topic-Häufigkeiten über die Zeit interessiert sind, wird die Variable mit dem Erscheinungstags des Posts in eine numerische Variable umgewandelt. Sie ist so skaliert, dass der aktuellste Post den Wert 0 hat. Diese Variable können wir dann als Prädiktor beim Schätzen des Structural Topic Model berücksichtigen. Unter Pruning versteht man das Entfernen von Features, die entweder in sehr wenigen oder in sehr vielen Dokumenten vorkommen. Dadurch können die Größe des Datensatzes und in der Folge die zum Schätzen der Modelle nötigen Ressourcen wesentlich reduziert werden. Inhaltlich sollte das Entfernen dieser Features wenig ändern: Features, die in sehr vielen Dokumenten vorkommen, tragen nicht zur Differenzierung zwischen den Dokumenten bei. Features, die nur in sehr wenigen Dokumenten vorkommen, tragen nicht zur Definition von Topics bei, da diese durch das regelmäßige gemeinsame Vorkommen in Dokumenten identifiziert werden. Siehe ausführlich Maier et al. (2018). Die Vorbereitung des Korpus und der Dokument-Feature-Matrix erfolgte mit Funktionen aus {quanteda}. Mit der Funktion corpus() wird der Datensatz in einen Text-Korpus umgewandelt. In diesem Zuge wird auch die numerische Datums-Variable erstellt. Die Variable mit dem Text des Posts duplizieren wir, damit sie zusätzlich als Meta-Datum für jeden Text gespeichert wird. Das wird später hilfreich sein, wenn wir die Ergebnisse einer Modellschätzung explorieren. custom_stopwords und relevant_ngrams zeigen die Stoppwörter und Wortkombinationen, die ausgeschlossen bzw. einbezogen werden. Letztere werden mit der Funktion dictionary() aus {quanteda} erstellt. Mit der Funktion dfm() wird der Korpus in eine Dokument-Feature-Matrix umgewandelt. Dabei werden die Standard-Schritte der Textaufbereitung durchgeführt. Sie besteht aus 12,369 Posts in den Zeilen und 41,385 Features in den Spalten. In jeder Zelle ist angegeben, wie häufig ein Feature in einem Dokument vorkommt. Mit der Funktion dfm_trim() wird das Pruning durchgeführt. Dabei werden alle Features, die in weniger als 0.5% oder mehr als 99% der Posts vorkommen, entfernt. Nach dem Pruning enthält die Matrix nur noch 1,150 Features. Zuletzt muss die Matrix in das von stm() benötigte Format konvertiert werden. Dabei werden zwei Posts gelöscht, die nach der Bereinigung kein einziges Feature mehr enthalten. Wichtig für den Bericht der Fallzahl in einer Publikation! Am Ende seht ihr eine einfache Beschreibung der häufigsten Features im Korpus als Tabelle und Wordcloud. # Erstellen des Korpus crps = d %&gt;% mutate(txt = post, # Duplizieren des Post-Texts für Meta-Daten date_num = as.numeric(postdate) - max(as.numeric(postdate))) %&gt;% # Numerische Datumsvariable corpus(text_field = &quot;post&quot;) # Erstellen des Korpus crps %&gt;% summary(n = 1) ## Corpus consisting of 12369 documents, showing 1 document: ## ## Text Types Tokens Sentences author postdate wc thread_title ## text1 27 29 2 zwerg-bayern 2018-04-06 26 HPV-Impfung ## txt ## Wenn Impfungen zu Todesfälle oder starken Nebenwirkungen in einzelnen Fällen führen, spricht dann für Immungschwäche oder schlummernder Krankheit. Ein gesunder Mensch stirbt nicht an der Impfung. ## date_num ## -457 # Stoppwörter custom_stopwords = c(&quot;ab&quot;, &quot;aber&quot;, &quot;ach&quot;, &quot;all&quot;, &quot;alle&quot;, &quot;allem&quot;, &quot;allen&quot;, &quot;aller&quot;, &quot;alles&quot;, &quot;als&quot;, &quot;also&quot;, &quot;am&quot;, &quot;an&quot;, &quot;andere&quot;, &quot;anderen&quot;, &quot;anderes&quot;, &quot;anders&quot;, &quot;auch&quot;, &quot;auf&quot;, &quot;aufs&quot;, &quot;aus&quot;, &quot;bei&quot;, &quot;beim&quot;, &quot;bin&quot;, &quot;bis&quot;, &quot;bist&quot;, &quot;bzw&quot;, &quot;da&quot;, &quot;dabei&quot;, &quot;dadurch&quot;, &quot;daher&quot;, &quot;dahin&quot;, &quot;damit&quot;, &quot;dann&quot;, &quot;das&quot;, &quot;dass&quot;, &quot;daß&quot;, &quot;dazu&quot;, &quot;dein&quot;, &quot;deine&quot;, &quot;deinem&quot;, &quot;deinen&quot;, &quot;deiner&quot;, &quot;dem&quot;, &quot;den&quot;, &quot;denen&quot;, &quot;denn&quot;, &quot;dennoch&quot;, &quot;der&quot;, &quot;deren&quot;, &quot;des&quot;, &quot;deshalb&quot;, &quot;deswegen&quot;, &quot;dich&quot;, &quot;die&quot;, &quot;dies&quot;, &quot;diese&quot;, &quot;diesem&quot;, &quot;diesen&quot;, &quot;dieser&quot;, &quot;dieses&quot;, &quot;dir&quot;, &quot;doch&quot;, &quot;dort&quot;, &quot;dran&quot;, &quot;drauf&quot;, &quot;drin&quot;, &quot;drüber&quot;, &quot;du&quot;, &quot;durch&quot;, &quot;durchaus&quot;, &quot;eh&quot;, &quot;ein&quot;, &quot;eine&quot;, &quot;einem&quot;, &quot;einen&quot;, &quot;einer&quot;, &quot;eines&quot;, &quot;einige&quot;, &quot;einigen&quot;, &quot;einiges&quot;, &quot;einmal&quot;, &quot;er&quot;, &quot;es&quot;, &quot;etc&quot;, &quot;etwas&quot;, &quot;euch&quot;, &quot;euer&quot;, &quot;eure&quot;, &quot;euren&quot;, &quot;für&quot;, &quot;fürs&quot;, &quot;gegen&quot;, &quot;gehabt&quot;, &quot;getan&quot;, &quot;gewesen&quot;, &quot;geworden&quot;, &quot;hab&quot;, &quot;habe&quot;, &quot;haben&quot;, &quot;habt&quot;, &quot;halt&quot;, &quot;hast&quot;, &quot;hat&quot;, &quot;hatte&quot;, &quot;hätte&quot;, &quot;hatten&quot;, &quot;hätten&quot;, &quot;her&quot;, &quot;hier&quot;, &quot;hin&quot;, &quot;hinter&quot;, &quot;ich&quot;, &quot;ihm&quot;, &quot;ihn&quot;, &quot;ihnen&quot;, &quot;ihr&quot;, &quot;ihre&quot;, &quot;ihrem&quot;, &quot;ihren&quot;, &quot;ihrer&quot;, &quot;im&quot;, &quot;in&quot;, &quot;ins&quot;, &quot;is&quot;, &quot;ist&quot;, &quot;ja&quot;, &quot;je&quot;, &quot;jede&quot;, &quot;jedem&quot;, &quot;jeden&quot;, &quot;jeder&quot;, &quot;jedes&quot;, &quot;jetzt&quot;, &quot;kann&quot;, &quot;kannst&quot;, &quot;kein&quot;, &quot;keine&quot;, &quot;keinem&quot;, &quot;keinen&quot;, &quot;keiner&quot;, &quot;können&quot;, &quot;könnt&quot;, &quot;konnte&quot;, &quot;könnte&quot;, &quot;könnten&quot;, &quot;mach&quot;, &quot;mache&quot;, &quot;machen&quot;, &quot;machst&quot;, &quot;macht&quot;, &quot;mal&quot;, &quot;man&quot;, &quot;manche&quot;, &quot;mein&quot;, &quot;meine&quot;, &quot;meinem&quot;, &quot;meinen&quot;, &quot;meiner&quot;, &quot;meines&quot;, &quot;mich&quot;, &quot;mir&quot;, &quot;mit&quot;, &quot;muss&quot;, &quot;müssen&quot;, &quot;musst&quot;, &quot;musste&quot;, &quot;müsste&quot;, &quot;mussten&quot;, &quot;na&quot;, &quot;nach&quot;, &quot;nachdem&quot;, &quot;naja&quot;, &quot;ne&quot;, &quot;nein&quot;, &quot;nem&quot;, &quot;nen&quot;, &quot;ner&quot;, &quot;nicht&quot;, &quot;nichts&quot;, &quot;nix&quot;, &quot;noch&quot;, &quot;nun&quot;, &quot;nur&quot;, &quot;ob&quot;, &quot;oder&quot;, &quot;ohne&quot;, &quot;ok&quot;, &quot;okay&quot;, &quot;raus&quot;, &quot;rein&quot;, &quot;rum&quot;, &quot;schon&quot;, &quot;sehr&quot;, &quot;sei&quot;, &quot;seid&quot;, &quot;sein&quot;, &quot;seine&quot;, &quot;seinem&quot;, &quot;seinen&quot;, &quot;seiner&quot;, &quot;selber&quot;, &quot;selbst&quot;, &quot;sich&quot;, &quot;sie&quot;, &quot;sind&quot;, &quot;so&quot;, &quot;solche&quot;, &quot;solchen&quot;, &quot;soll&quot;, &quot;sollen&quot;, &quot;sollte&quot;, &quot;sollten&quot;, &quot;solltest&quot;, &quot;somit&quot;, &quot;sondern&quot;, &quot;sonst&quot;, &quot;sowas&quot;, &quot;soweit&quot;, &quot;tun&quot;, &quot;tut&quot;, &quot;über&quot;, &quot;um&quot;, &quot;und&quot;, &quot;uns&quot;, &quot;unser&quot;, &quot;unsere&quot;, &quot;unserem&quot;, &quot;unseren&quot;, &quot;unserer&quot;, &quot;unter&quot;, &quot;usw&quot;, &quot;viel&quot;, &quot;viele&quot;, &quot;vielen&quot;, &quot;vieles&quot;, &quot;vom&quot;, &quot;von&quot;, &quot;vor&quot;, &quot;war&quot;, &quot;wäre&quot;, &quot;waren&quot;, &quot;wären&quot;, &quot;wars&quot;, &quot;was&quot;, &quot;weder&quot;, &quot;weg&quot;, &quot;wegen&quot;, &quot;weil&quot;, &quot;weiter&quot;, &quot;weitere&quot;, &quot;welche&quot;, &quot;welchen&quot;, &quot;welcher&quot;, &quot;welches&quot;, &quot;wenn&quot;, &quot;wenns&quot;, &quot;wer&quot;, &quot;werd&quot;, &quot;werde&quot;, &quot;werden&quot;, &quot;wie&quot;, &quot;wieder&quot;, &quot;wieso&quot;, &quot;will&quot;, &quot;willst&quot;, &quot;wir&quot;, &quot;wird&quot;, &quot;wirst&quot;, &quot;wo&quot;, &quot;wobei&quot;, &quot;wollen&quot;, &quot;wollte&quot;, &quot;wollten&quot;, &quot;worden&quot;, &quot;wurde&quot;, &quot;würde&quot;, &quot;wurden&quot;, &quot;würden&quot;, &quot;z.b&quot;, &quot;zb&quot;, &quot;zu&quot;, &quot;zum&quot;, &quot;zur&quot;, &quot;zwar&quot;, &quot;zwischen&quot;) # Kombinationen von Wörtern relevant_ngrams = dictionary(list( &quot;trotz_impfung&quot; = &quot;trotz impfung&quot;, &quot;grippe_impfen&quot; = &quot;grippe impfen&quot;, &quot;mmr_impfung&quot; = &quot;mmr impfung&quot;, &quot;hepatitis_b&quot; = &quot;hepatitis b&quot;, &quot;gut_vertragen&quot; = &quot;gut vertragen&quot;, &quot;6fach_impfung&quot; = &quot;6fach impfung&quot;, &quot;6_fach&quot; = &quot;6 fach&quot;, &quot;6_fach_impfung&quot; = &quot;6 fach impfung&quot;, &quot;meningokokken_b&quot; = &quot;meningokokken b&quot;, &quot;gute_besserung&quot; = &quot;gute besserung&quot;, &quot;6-fach_impfung&quot; = &quot;6-fach impfung&quot;, &quot;erhöhte_temperatur&quot; = &quot;erhöhte temperatur&quot;, &quot;kein_fieber&quot; = &quot;kein fieber&quot;, &quot;kein_problem&quot; = &quot;kein problem&quot;, &quot;keine_ahnung&quot; = &quot;keine ahnung&quot;, &quot;keine_impfung&quot; = &quot;keine impfung&quot;, &quot;nicht_geimpft&quot; = &quot;nicht geimpft&quot;, &quot;nicht_impfen&quot; = &quot;nicht impfen&quot;, &quot;nicht_zu_impfen&quot; = &quot;nicht zu impfen&quot;, &quot;selbst_entscheiden&quot; = &quot;selbst entscheiden&quot; )) # Erstellen einer Dokument-Feature-Matrix aus dem Korpus impf_dfm = crps %&gt;% dfm(stem = FALSE, tolower = TRUE, remove_punct = TRUE, remove = custom_stopwords, remove_url = TRUE, verbose = TRUE, thesaurus = relevant_ngrams) ## Creating a dfm from a corpus input... ## ...lowercasing ## ...found 12,369 documents, 41,638 features ## ...applying a dictionary consisting of 20 keys ## ...removed 286 features ## ...complete, elapsed time: 3.2 seconds. ## Finished constructing a 12,369 x 41,372 sparse dfm. impf_dfm ## Document-feature matrix of: 12,369 documents, 41,372 features (99.9% sparse) and 6 docvars. ## features ## docs TROTZ_IMPFUNG GRIPPE_IMPFEN MMR_IMPFUNG HEPATITIS_B GUT_VERTRAGEN ## text1 0 0 0 0 0 ## text2 0 0 0 0 0 ## text3 0 0 0 0 0 ## text4 1 0 0 0 0 ## text5 0 0 0 0 0 ## text6 0 0 0 0 0 ## features ## docs 6FACH_IMPFUNG 6_FACH 6_FACH_IMPFUNG MENINGOKOKKEN_B GUTE_BESSERUNG ## text1 0 0 0 0 0 ## text2 0 0 0 0 0 ## text3 0 0 0 0 0 ## text4 1 0 0 0 0 ## text5 0 0 0 0 0 ## text6 0 0 0 0 0 ## [ reached max_ndoc ... 12,363 more documents, reached max_nfeat ... 41,362 more features ] # Pruning impf_dfm = impf_dfm %&gt;% dfm_trim(max_docfreq = 0.99, min_docfreq = 0.005, docfreq_type = &quot;prop&quot;) impf_dfm ## Document-feature matrix of: 12,369 documents, 1,150 features (98.2% sparse) and 6 docvars. ## features ## docs TROTZ_IMPFUNG GRIPPE_IMPFEN MMR_IMPFUNG GUT_VERTRAGEN 6_FACH ## text1 0 0 0 0 0 ## text2 0 0 0 0 0 ## text3 0 0 0 0 0 ## text4 1 0 0 0 0 ## text5 0 0 0 0 0 ## text6 0 0 0 0 0 ## features ## docs MENINGOKOKKEN_B GUTE_BESSERUNG ERHÖHTE_TEMPERATUR KEIN_FIEBER ## text1 0 0 0 0 ## text2 0 0 0 0 ## text3 0 0 0 0 ## text4 0 0 0 0 ## text5 0 0 0 0 ## text6 0 0 0 0 ## features ## docs KEIN_PROBLEM ## text1 0 ## text2 0 ## text3 0 ## text4 0 ## text5 0 ## text6 0 ## [ reached max_ndoc ... 12,363 more documents, reached max_nfeat ... 1,140 more features ] # Überblick: Die häufigsten Features im Korpus impf_dfm %&gt;% colSums() %&gt;% enframe() %&gt;% arrange(desc(value)) %&gt;% slice(1:20) %&gt;% kable() name value impfung 4519 impfen 4356 kind 3690 lassen 3257 immer 2705 mehr 2594 kinder 2503 gibt 2184 geimpft 2177 impfungen 2174 gut 2115 hallo 1898 einfach 1767 lg 1720 2 1704 bekommen 1585 ganz 1584 erst 1558 geht 1492 arzt 1411 # als (beliebte, wenn auch nur mittel informative) Wordcloud impf_dfm %&gt;% textplot_wordcloud() # Umwandeln der dfm in das Format für stm impf_stm = impf_dfm %&gt;% quanteda::convert(to = &quot;stm&quot;) ## Warning in dfm2stm(x, docvars, omit_empty = TRUE): Dropped empty document(s): ## text12225, text12271 Literatur "],
["modellspezifikation-modellvergleich-und-modellauswahl.html", "Abschnitt 3 Modellspezifikation, Modellvergleich und Modellauswahl 3.1 Modellspezifikation 3.2 Modellvergleich und -auswahl", " Abschnitt 3 Modellspezifikation, Modellvergleich und Modellauswahl 3.1 Modellspezifikation Die Funktion stm() aus dem gleichnamigen Paket bietet zahlreiche Möglichkeiten, Details der Modellspezifikation und -schätzung anzupassen. Wir beschränken uns im Folgenden auf drei wesentliche Einstellungen: K: Die Zahl der Topics. prevalence: Eine Formel zur Vorhersage der Topic-Prävalenzen init.type: Wie soll der Startpunkt für die Modellschätzung gewählt werden? Zu weiteren Details siehe für einen Überblick ?stm und Roberts, Stewart, and Tingley (2019) für eine ausführliche Erläuterung. Eine Modell-Spezifikation könnte z.B. so aussehen: modelfit = stm(documents = impf_stm$documents, vocab = impf_stm$vocab, data = impf_stm$meta, K = 10, prevalence = ~s(date_num), init.type = &quot;Spectral&quot;) Mit den ersten drei Inputs übergeben wir die Daten aus der im letzten Abschnitt erstellten Dokument-Feature-Matrix. Mit K geben wir an, wie viele Themen es geben soll. Mit dieser Syntax würde ein Modell mit \\(k = 10\\) Themen geschätzt. Wie wir bei der Wahl eines geeigneten \\(k\\) vorgehen können, ist Thema des folgenden Unterabschnitts. Mit der Formel zu prevalence geben wir an, welche Dokument-Variablen mit dem Auftreten der Topics zusammenhängen. In der Formel wird die abhängige Variable vor der Tilde (~) frei gelassen. Es wird immer der Zusammenhang mit dem Auftreten von allen \\(k\\) Topics geschätzt. In diesem Beispiel schätzen wir, wie sich das Auftreten der Topics über den Untersuchungszeitraum hinweg verändert. Details zum Schätzen von Zusammenhängen mit Kovariaten folgen in Kapitel 4. Mit init.type wird angegeben, wie stm() die Ausgangswerte für die Modellschätzung bestimmen soll. Die Default-Einstellung ist “Spectral”. Ich empfehle diese Einstellung aus folgenden Gründen: Sie ist deterministisch, d.h., sie führt gegeben derselben Daten und desselben Modells immer zu derselben Lösung. So wird die Reproduzierbarkeit sichergestellt. Sie ist effizient, d.h., dass von diesem Startpunkt aus relativ schnell die finale Lösung gefunden wird. Wenn eine andere Einstellung für die Ausgangswerte gewählt wird, müssen mehrere Schätzungen für eine Spezifikation durchgeführt werden. Nur so kann geprüft werden, ob die Ausgangswerte das Ergebnis beeinflussen. Allgemein muss beachtet werden, dass die Schätzung eines Structural Topic Model mit stm() trotz der Effizienz der Implementierung sehr rechenintensiv ist. Die Schätzung des oben beschriebenen Modells dauert auf meinem recht leistungsfähigen Notebook bereits ca. eine Minute. Es empfiehlt sich daher, die Modelle immer in neue Objekte zu speichern und diese ggf. direkt auf der Festplatte zu sichern. Um die Berechnungszeiten in diesem Workshop kurz zu halten, stelle ich die Ergebnisse der Modellschätzungen über das LMS zur Verfügung. Wenn ihr diese herunterladet und in den Ordner “data” kopiert, muss das Modell nicht neu geschätzt werden. 3.2 Modellvergleich und -auswahl 3.2.1 Allgemeines Vorgehen Eine zentrale Frage ist die Wahl eines geeigneten \\(k\\), also der Zahl von Topics, die in den Dokumenten identifiziert werden sollen. Wichtig ist zuerst die Feststellung, dass es in der angewandten Analyse kein per se richtiges oder falsches \\(k\\) gibt. Wie viele Topics nützlich sind, hängt von Umfang von Zusammensetzung des Materials und vom substantiellen Forschungsinteresse ab. Um ein geeignetes \\(k\\) zu finden, gehen wir in der Regel modellvergleichend vor. Wir schätzen Modelle mit unterschiedlich vielen Topics und prüfen dann, welche Modelle besser zu den Daten und zum Forschungsinteresse passen. Hinweise für einen allgemeinen Ausgangspunkt, in welchem Bereich nützliche \\(k\\) zu finden sein könnten, liefert die Paket-Hilfe: The most important user input in parametric topic models is the number of topics. There is no right answer to the appropriate number of topics. More topics will give more fine-grained representations of the data at the potential cost of being less precisely estimated. […] For short corpora focused on very specific subject matter (such as survey experiments) 3-10 topics is a useful starting range. For small corpora (a few hundred to a few thousand) 5-50 topics is a good place to start. Beyond these rough guidelines it is application specific. Previous applications in political science with medium sized corpora (10k to 100k documents) have found 60-100 topics to work well. For larger corpora 100 topics is a useful default size. Of course, your mileage may vary. — ?stm Hier werden zwei wichtige Kriterien, die unser Nachdenken über die Spannweite von zu Berücksichtigten \\(k\\) leiten können, deutlich: Quantität des Materials: Je mehr Dokumente, desto mehr Topics. Varianz im Inhalt: Je mehr inhaltliche Varianz, desto mehr Topics (an einem Beispiel: für 10k Nachrichtenbeiträge aus dem Wirtschaftsteil brauchen wir weniger Topics als für 10k Nachrichtenbeiträge, die aus allen Ressorts kommen). Im vorliegenden Fall haben wir einen kleinen bis mittleren Korpus (ca. 13k Dokumente, die größtenteils recht kurz sind). Wir können von einer mittleren inhaltlichen Varianz ausgehen. Einerseits haben wir Posts bewusst danach ausgewählt, dass sie sich mit dem Thema Impfen beschäftigen, was die Varianz einschränkt. Andererseits wissen wir, dass in Online-Foren die verschiedensten Perspektiven auf dieses Thema vorkommen können, was für Varianz sorgt. Wir gehen im Folgenden in mehreren Schritten vor: Um eine allgemeine Orientierung zu erhalten, in welcher Range Modelle zu finden sind, die gut zu den Daten passen, schätzen wir 10 Modelle von \\(k = 10\\) bis \\(k = 100\\) mit einem Abstand von jeweils 10 Topics. Diese Modelle vergleichen wir anhand von einigen statistischen Maßen, um die Zahl der Kandidatenmodelle einzuschränken. Wir interpretieren die besten Modelle substantiell und entscheiden, welche Topic-Anzahl für das Forschungsinteresse hilfreicher scheint. Wir schätzen weitere Modelle basierend auf den Ergebnissen aus 2) mit kleineren Abständen zwischen den \\(k\\). Wir prüfen, wie sich die Topics verändern. Zudem achten wir darauf, ob bei Modellen mit größeren \\(k\\) interessante Topics hinzukommen oder ob sich mehr Ambivalenzen zeigen. Wir entscheiden uns für ein Modell. Da das Schätzen der Modelle recht lange dauert, parallelisieren wir die Berechnung. Dazu nutze ich das Paket furrr. Es sei an dieser Stelle darauf hingewiesen, dass das Paket vor allem unter Windows mit RStudio für Probleme sorgen kann. Es ist daher empfehlenswert, das Skript zum Schätzen und Speichern der Modelle in der Konsole oder im Terminal auszuführen. Noch schneller geht es für diesen Workshop, die bereits geschätzten Modelle aus dem LMS zu laden. 3.2.2 Quantitativer Vergleich der ersten Modelle # 1) Modelle mit K = 10, ..., K = 100 # Modelle schätzen bzw. laden if (file.exists(&quot;R/data/models10_100.rds&quot;)) { # Schneller: Modelle aus LMS laden many_models = read_rds(&quot;R/data/models10_100.rds&quot;) } else { # Vorsicht: Schätzen dauert auf meinem MacBook Pro 2020 i9 32 GB RAM 10 Minuten Ks = seq(10, 100, by = 10) tic() plan(multiprocess(workers = 10)) many_models = tibble(K = Ks) %&gt;% mutate(topic_model = future_map(K, ~stm(documents = impf_stm$documents, vocab = impf_stm$vocab, data = impf_stm$meta, init.type = &quot;Spectral&quot;, K = ., verbose = FALSE), .progress = TRUE)) plan(sequential) toc() saveRDS(many_models, &quot;R/data/models10_100.rds&quot;) } # Quantitative Indikatoren der Modellqualität berechnen # Inspired by https://juliasilge.com/blog/evaluating-stm/ if (file.exists(&quot;R/data/eval10_100.rds&quot;)) { # Schneller: Modellevaluation aus LMS laden model_eval = read_rds(&quot;R/data/eval10_100.rds&quot;) } else { # Evaluation dauert auf meinem MacBook Pro 2020 i9 32 GB RAM 24 Sekunden tic() heldout = make.heldout(documents = impf_stm$documents, vocab = impf_stm$vocab) plan(multiprocess(workers = 10)) model_eval = many_models %&gt;% mutate(exclusivity = future_map(topic_model, exclusivity), semantic_coherence = future_map(topic_model, semanticCoherence, impf_stm$documents), eval_heldout = future_map(topic_model, eval.heldout, heldout$missing), residual = future_map(topic_model, checkResiduals, impf_stm$documents), residuals = future_map_dbl(residual, &quot;dispersion&quot;), held_out_likelihood = future_map_dbl(eval_heldout, &quot;expected.heldout&quot;)) %&gt;% select(-topic_model) plan(sequential) toc() saveRDS(model_eval, &quot;R/data/eval10_100.rds&quot;) } # Exklusivität und Semantische Kohärenz (Mean, Median) model_eval %&gt;% select(K, Exclusivity = exclusivity, Coherence = semantic_coherence) %&gt;% mutate_at(-1, .funs = list(Mean = ~map_dbl(.x, mean), Median = ~map_dbl(.x, median))) %&gt;% select_if(negate(is.list)) %&gt;% gather(metric, value, -K) %&gt;% separate(metric, c(&quot;measure&quot;, &quot;metric&quot;), &quot;_&quot;) %&gt;% spread(measure, value) %&gt;% ggplot(aes(Coherence, Exclusivity, label = K)) + geom_text() + facet_wrap(&quot;metric&quot;) # Exklusivität und Semantische Kohärenz von k = 40, 50 model_eval %&gt;% filter(K %in% c(40, 50)) %&gt;% select(K, Exclusivity = exclusivity, Coherence = semantic_coherence) %&gt;% unnest(c(Exclusivity, Coherence)) %&gt;% ggplot(aes(Coherence, Exclusivity, color = factor(K))) + geom_point() # Held-out-likelihood und multinomiale Residuen model_eval %&gt;% select(K, `Held-out likelihood (higher is better)` = held_out_likelihood, `Multinomial dispersion of residuals (lower is better)` = residuals) %&gt;% gather(measure, value, -K) %&gt;% ggplot(aes(K, value, label = K)) + geom_line() + geom_text() + facet_wrap(&quot;measure&quot;, scales = &quot;free_y&quot;, ncol = 1) + labs(x = &quot;Number of topics (K)&quot;, y = NULL) # Speichern der Modelle mit 30 und 40 Topics für qualitative Analyse # Benennung und Datenstruktur für stminsights out = impf_stm m30 = many_models$topic_model[[3]] m40 = many_models$topic_model[[4]] m60 = many_models$topic_model[[6]] # save(out, m30, m40, m60, file = &quot;R/data/models30_40_60.rdata&quot;) Wir betrachten zuerst die semantische Kohärenz und die Exklusivität der Topics in den Modellen. Beide Metriken sind Eigenschaften der einzelnen Themen. In der ersten Grafik sind daher der Mittelwerte und die Mediane aller Themen in einem Modell dargestellt. Die absoluten Werte beider Metriken haben keine substantielle Bedeutung. Von Interesse ist der Vergleich der Modelle. Je höher die Exklusivität, desto geringer ist die Wahrscheinlichkeit, dass die typischsten Terme eines Topics in anderen Topics vorkommen (Roberts et al. 2014). Je höher die semantische Kohärenz, desto wahrscheinlicher kommen die typischsten Wörter eines Topics gemeinsam in einem Dokument mit diesem Topic vor (Mimno et al. 2011). Zwischen den beiden Metriken besteht in der Regel ein negativer Zusammenhang. Daher ist es notwendig, eine Balance zwischen beiden zu finden. Im vorliegenden Beispiel können wir zuerst die Modelle mit \\(k \\ge 80\\) ausschließen. Obwohl sie mehr Topics benötigen, sind ihre Topics im Mittel weniger exklusiv und weniger kohärent als die Topics der Modelle mit \\(k = 60\\) oder \\(k = 70\\). Eine Erklärung dafür kann sein, irgendwann zwischen dem 70. und dem 80. Topic keine substantiell neuen Aspekte mehr im Korpus zu finden sind. Die neuen Topics sind dann redundant zu schon bestehenden Topics. Ebenso können wir für die meisten Zwecke die Modelle mit \\(k \\le 20\\) vernachlässigen, da die mittlere Exklusivität ihrer Topics wesentlich geringer ist als die der übrigen Modelle. Das Modell mit \\(k = 20\\) könnte vielleicht infrage kommen, wenn wir Sparsamkeit sehr hoch gewichten, also den Korpus durch möglichst wenige Topics beschreiben wollen. Schließlich ist das Modell mit \\(k = 50\\) nicht besonders attraktiv. Die Topics sind im Mittel nur weniger kohärent, aber nicht exklusiver, als die Topics des Modells mit \\(k = 40\\). Der Detailvergleich der beiden Modelle in der nächsten Abbildung, in der jedes Topic mit einem Punkt dargestellt ist, zeigt, dass dies vor allem auf vier Topics zurück geht, die weniger kohärent sind, ohne eine besonders gute Exklusivität aufzuweisen. Gegeben der Metriken semantische Kohärenz und Exklusivität spricht einiges dafür, entweder Modelle mit ca. 60-70 Topics oder Modelle mit 30-40 Topics weiter zu verfolgen. Die held-out likelihood und die Dispersion der multinomialen Residuen sind zwei Metriken, die die Abweichung des Modells von den Daten quantifizieren. Auch sie sind wieder im relativen Modellvergleich zu interpretieren. Die held-out likelihood gibt Auskunft darüber, wie gut das Vorkommen von Wörter in einem Dokument, das nicht zum Schätzen des Modells genutzt wurde, vorhergesagt werden kann. Mit der Dispersion der multinomialen Residuen wird die Abweichung der durch das Modell vorhergesagten von den beobachteten Wörtern in den Dokumenten auf Basis des gesamten Datensatzes quantifiziert. Ein Wert von 1 wäre ideal, wird in angewandten Beispielen mit echten Texten aber kaum erreicht. Beide Metriken sind eng verwandt und legen in der Regel ähnliche Entscheidungen nahe. Das Modell mit \\(k = 50\\) ist nach beiden Metriken gut geeignet. Nach der held-out likelihood sind die Modelle mit \\(k \\ge 80\\) noch etwas besser - diese haben sich aber in der semantischer Kohärenz und Exklusivität nicht sonderlich gut bewährt. Die nach semantischer Kohärenz und Exklusivität besten Modelle liegen nach diesen beiden Metriken etwa gleich auf. Die Befunde sind auch für die didaktischen Zwecke dieses Workshops gut geeignet. Es wird klar, dass die Modellwahl sich nicht einfach automatisieren lässt. Die quantitativen Kriterien helfen und lediglich, den Raum möglicher Modelle einzuschränken. Für mich kommen auf Basis der berichteten Metriken Modelle mit zwischen 30 und 70 Topics infrage. Wenn man an einer besonders sparsamen Lösung interessiert ist, könnte man auch \\(k = 20\\) in Erwägung ziehen. Wenn man besonders detailliertere Lösungen sucht, sind auch die Modelle mit \\(k \\ge 80\\) nicht ausgeschlossen. Hier müsste man dann die Differenzierungen zwischen den Topics in der Tiefe untersuchen und klarstellen. Im nächsten Schritt vergleichen wir die Modelle mit 30 und mit 60 Topics, um zu verstehen, welche Konsequenzen es für die inhaltliche Interpretation hat, ein eher sparsames oder ein eher detailliertes Modell zu wählen. Nach dieser Entscheidung können wir dann weiter überlegen, welches der eher sparsamen bzw. detaillierten Modelle für unser Forschungsinteresse besser geeignet ist. 3.2.3 Qualitativer Vergleich der Modelle mit 30 und 60 Topics Um die Ergebnisse eines Topic Model interpretieren zu können, müssen wir zuerst verstehen, wie das Ergebnis eines Topic Models aussieht. Es enthält im wesentlichen zwei zweidimensionale Vektoren von Koeffizienten. beta: Für jedes Topic die Wahrscheinlichkeit, dass ein Dokument, in dem das Feature vorkommt, das Topic hat. Jedes Topic erhält für jedes Feature einen beta-Koeffizienten zwischen 0 und 1, die zusammen 1 ergeben. theta bzw. gamma (uneinheitlich benannt): Für jedes Dokument die Wahrscheinlichkeit, dass das Dokument das Topic enthält. Jedes Dokument erhält für jedes Topic einen theta- bzw. gamma-Koeffizienten zwischen 0 und 1, die zusammen 1 ergeben. Um die Bedeutung der Topics zu interpretieren, betrachten wir daher zwei Modell-Outputs. Die Features, die am typischsten für Dokumente mit einem Topic sind, also die Features mit den höchsten beta-Koeffizienten (ggf. nach Korrekturen für die Verteilung der Features im gesamten Korpus). Die Dokumente, die mit der größten Wahrscheinlichkeit ein Thema enthalten (manchmal auch interpretiert als zum größten Anteil aus einem Thema bestehen), also die Dokumente mit den höchsten gamma- bzw. theta-Koeffizienten. Zur qualitativen Interpretation der Modelle anhand dieser Outputs kann ich das Paket {stminsights} empfehlen — vor allem denjenigen, die lieber mit einer intuitiven grafischen Benutzeroberfläche als direkt mit R arbeiten. Aber auch ich selbst schätze das Tool für einen schnellen Überblick über mehrere Modelle. Leider ist es zurzeit nicht einfach, {stminsights} direkt zum Laufen zu bringen, da einige Pakete, auf denen es aufbaut, Bugs bzw. Kompabilitätsprobleme haben. Eine Anleitung, wie das Paket zurzeit installiert werden kann, gibt es hier: Important note: The shiny app for the CRAN release of stminsights does currently not work properly due to bugs introduced by recent changes in the Shiny package […]. Please use the Github version of stminsights for now. This will require the development version of Shiny which can be installed by running devtools::install_github(‘rstudio/shiny’). You can download and install the latest development version of stminsights by running devtools::install_github(‘cschwem2er/stminsights’) — https://github.com/cschwem2er/stminsights Zur Vorbereitung der Analyse mit {stminsights} müssen die Objekte der geschätzten Modelle (hier: m30, m40 [brauchen wir etwas später] und m60) und die Daten, die zur Modellschätzung verwendet wurden (hier: out = ìmpf_stm), als .rdata Datei gespeichert werden. Dabei muss das Daten-Objekt unbedingt den Namen out haben. Damit der Text der Dokumente in {stminsights} angezeigt werden kann, muss dieser zusätzlich als Variable in den Meta-Daten enthalten sein (siehe Abschnitt zur Datenaufbereitung). Durch das ausführen der Funktion run_stminsights() wird eine grafische Benutzeroberfläche im Browser gestartet, mit dem die qualitative Analyse durchgeführt wird. Eine Beschreibung der Oberfläche findet ihr als Video im LMS [kommt nach Fertigstellen des Textmaterials]. run_stminsights() Wer die Interpretation der Topics lieber in R durchführt, kann z.B. den folgenden Code verwenden und anpassen. Wir sammeln zuerst die 20 typischsten Texte für jedes Topic in einem Datensatz. Dabei bereiten wir die Texte auch für eine Ausgabe in der Konsole vor. Der Parameter “gamma” oder “theta” (uneinheitlich benannt, aber derselbe Parameter) kann mit tidytext::tidy() extrahiert werden. Dann werden die Texte aus den Ursprungsdaten zugespielt. Mit stm::labelTopics() können wird die typischsten Features für ein Topic anzeigen. n bestimmt die Zahl der Features, topics das Topic, das wir gerade beschreiben möchten. Es werden die typischsten Features nach vier verschiedenen Kriterien ausgegeben (Diese werden auch in {stminsights} angezeigt): Highest Prob zeigt die Features, die mit der größten Wahrscheinlichkeit in Dokumenten mit einem Topic vorkommen (= Features mit den höchsten beta-Koeffizienten für das Topic). Dabei wird die Gesamthäufigkeit der Features im Korpus nicht berücksichtigt. Wörter, die allgemein sehr häufig vorkommen, sind nach dieser Metrik typisch für verschiedene Topics. Die anderen drei Metriken versuchen, diese Schwäche auf verschiedene Art und Weise zu korrigieren. FREX steht für most frequent and exclusive Features. Hier werden die Features aufgelistet, die möglichst typisch für ein Dokument mit einem Topic, aber möglichst nicht sehr typisch für Dokumente mit anderen Topics sind. Siehe ?calcfrex für technische Details. Die Lift-Metrik setzt die Wahrscheinlichkeit, dass ein Feature in einem Dokument mit einem Topic vorkommt, zur Wahrscheinlichkeit, dass ein Feature in einem beliebigen Dokument vorkommt, ins Verhältnis. Siehe ?calclift für technische Details. Score gewichtet für die Wahrscheinlichkeit, mit der ein Feature in Dokumenten mit einem anderen Topics vorkommt. Siehe ?calcscore für technische Details. Mit den Auszügen aus dem Datensatz typischer Dokumente (gefiltert nach Topic) können wir diese Features zudem im Kontext der gesamten Texte sehen. Im abschließenden Datensatz topic_labels halten wir für jedes Topic ein aussagekräftiges, möglichst kurzes Label fest, das wir jetzt zur eigenen Übersicht und später auch zur Ergebnisdarstellung verwenden. Zusätzlich empfehle ich, eine kurze Zusammenfassung für jedes Topic auf einem Medium eigener Wahl (präferierte analoger oder digitaler Notizzettel) festzuhalten. # Laden der Modelle load(&quot;R/data/models30_40_60.rdata&quot;) # Beispiel für das Modell mit k = 30 # Erstellen eines Datensatzes mit den typischsten Dokumenten top_docs = tidy(m30, &quot;gamma&quot;) %&gt;% arrange(desc(gamma)) %&gt;% group_by(topic) %&gt;% slice(1:20) %&gt;% mutate(rank = 1:20) %&gt;% left_join(mutate(select(out$meta, txt), document = 1:n())) %&gt;% mutate(out = paste0(round(gamma, 2), &quot;: &quot;, txt)) # Ausgabe der typischen Feature für ein Topic (hier Topic 1) m30 %&gt;% labelTopics(n = 10, topics = 1) ## Topic 1 Top Words: ## Highest Prob: 2, 3, 1, monate, erst, monaten, 4, 6, alt, 5 ## FREX: 3, 2, monate, monaten, 6, 4, 1, monat, +, 5 ## Lift: österreich, zecken, monat, fach, 3, 2, monate, 6, +, monaten ## Score: österreich, 2, 3, monate, 1, monaten, 4, alt, 6, + # Ausgabe der typischen Texte für ein Topic (hier Topic 1) top_docs %&gt;% filter(topic == 1) %&gt;% filter(rank %in% 1:10) %&gt;% .$out %&gt;% str_squish() %&gt;% cat(sep = &quot;\\n\\n&quot;) ## 0.62: da fängt der 12 lebensmonat an, ein Jahr alt ist dein Baby dann erst mit vollendetem 12. Lebensmonat.... meine Tochter wird jetzt am 24.07 12 Monate alt, und am 24.08 wird sie 1 Jahr....da ist also der 12 lebensmonat vollendet :) ## ## 0.59: Man spricht erst sobald der Monat vollendet ist vom z.B. 12. Monat. Deine Tochter ist also einen Monat vor ihrem 1. Geburtstag ELF Monate alt und nicht 12... Klar, sie befindet sich ab dem 24.07. im zwölften Lebensmonat, ist aber noch keine 12 Monate alt, sondern 11. Kann etwas verwirrend sein, ich weiß... Denn sie ist am Tag ihrer Geburt ja auch nicht einen Monat alt... Sondern erst einen Monat nach ihrer Geburt Ich bin aktuell 30 Jahre alt und befinde mich somit in meinem 31. Lebensjahr. So wird das immer gerechnet :) ## ## 0.59: In Österreich wird im 3., 5. und 12. Monat geimpft. Ich bin auf einer anderen Seite auch fündig geworden. Dieses Schema nennt sich 2+1. Das in Deutschland von der Stiko empfohlene Schema nennt sich 3+1. Beide Schemata werden wohl in Deutschland als Grundimmunisierung anerkannt, da das Schema 2+1 einen ähnlichen bzw. gleichen Effekt hat wie 3+1. Die Stiko empfiehlt halt das 3+1 Schema. ## ## 0.53: so mein ich ja :) sie befindet sich ab dem Tag ihrer Geburt ja im erst Lebensmonat und nicht im nullsten Lebensmonat :D und ab dem 24.07 ist sie im 12. Öebensmonat.... wenn mich jemand Frägt wie alt sie ist sag ich auch nich 12 Monate alt sonder sie wird Ende August 1 Jahr alt...aber prinzipiell, so wird es auch bei Urbia angezeigt ist sie ab 24.07 im 12 lebensmonat....und wenn der vollendet ist, wird sie 1... :) ## ## 0.51: Wir mussten 2 Monate auf den Termin beim Kardiologen warten. Hatten den Termin dann als er jetzt 6 Monate alt war. Das Herzgeräusch sind 2 Sehnenfäden, aber er hat auch ein Mini Loch. Wir müssen erst wieder zur Kontrolle, wenn er 3 Jahre ist. ## ## 0.5: Korrektur: Bei der 2+1 Impfung sind ca 84% nach der 2 . Impfung immun. Bei der 3+1 Impfung sind ca 95 % nach der 3. Impfung immun. Und die ersten beiden bzw. 3 Impfungen werden ja in einen Zeitraum von 2 Monaten verabreicht. Also durchaus noch abzuwarten, wenn du dir da Sorgen machst. ## ## 0.47: hallo rebecca es gibt auch &quot;Mittelwege&quot;, zB das 2+1-Schema. Man beginnt erst mit 3 Monaten und spart sich eine Impfdosis. (Normalerweise ist ja 3 x innerhalb des 1. Lebensjahres und 1 x zur Auffrischung). Die Immunantwort ist am Ende wohl identisch, das Risiko besteht darin, dass man eben 1 Monat später beginnt und damit der Impfschutz 1 monat später beginnt. Wobei das größte Risiko hier der Keuchhusten ist. Das 2+1-Schema ist zB in Österreich Standard. ## ## 0.47: Unter 2 Jahren muss 2x geimpft werden. Mein Sohn ist 6 Monate. Wird am Freitag geimpft und dann in 2 Monaten wieder. Die Dritte erfolgt wenn er 2 Jahre alt ist oder 3 Jahre. Du musst alle 3 Impfungen bezahlen jeweils 120 Euro oder ein bisschen mehr. Die AOK Niedersachsen wird diese 3 Impfungen jeweils mit 80% mir erstatten. Die AOK Sachsen anscheinend alle 3 zu 100%. ## ## 0.44: Dann wär er aber 1 Monat vor Geburtstag 12 Monate : Wenn du die Wochen zählst darfst du halt nicht 4 Wochen mit 1 Monat gleich setzen. ## ## 0.43: Darf ich über die Fragestellung hinaus fragen, wie genau der Ablauf dann genau ist. Befasse mich in den ersten Zügen mit dem Thema Impfen. Es wird also in den ersten 12 Monaten 2x 5fach geimpft und dann 1x bis zum 24 Monat? Kann man das system 2+1 auch auf 6fach-impfung übertragen? Und auf +pneumokokken? # Datensatz für Topic Labels # Bis Topic 30 weiterführen m30_topic_labels = tribble( ~topic, ~label, 1, &quot;Alter von Babies&quot;, 2, &quot;label B&quot;, 3, &quot;label C&quot; ) Die Labels aus meiner vorläufigen Interpretation finden sich in R/topic_labels.R (hier nicht dargestellt). Die meisten Topics des Modells mit \\(k = 30\\) erwiesen sich als gut interpretierbar. An der einen oder anderen Stelle schien es aber so, als seien mehrere Aspekte der Impfdiskussionen in einem Topic vermischt. Dies könnte auf eine etwas größere Topic-Anzahl hindeuten. Die Interpretation der Topics des Modells mit \\(k = 60\\) fiel mir wesentlich schwerer. Die typischen Features vieler Topics waren zwar exklusiv für das jeweilige Topic, waren aber nicht direkt als ein Aspekt Diskussionen zum Thema impfen zu interpretieren. Häufig scheinen hier auch Terme, die z.B. aus Gründen der Formulierung häufiger zusammen auftauchen, ein Topic zu bilden. Der qualitative Vergleich der beiden Modelle legt für mich nahe, dass ein nützliches Modell eher in der Region etwas über \\(k = 30\\) zu finden ist als bei \\(k = 60\\). Der Grund dafür ist, dass ich meist Modelle mit möglichst vielen interpretierbare Topics bevorzuge. Wenn — wie in diesem Beispiel — die Dokumente bereits so ausgewählt wurden, dass die meisten Dokumente prinzipiell für das Forschungsinteresse relevant sind, halte ich es für sinnvoll, über einen möglichst großen Anteil der inhaltlichen Varianz in diesen Dokumenten etwas auszusagen. Topics, die aus der Analyse ausgeschlossen werden müssen, helfen nicht dabei, die Inhalte der Dokumente zu beschreiben und zu verstehen. Beachtet aber, dass das nicht die einzige sinnvolle Vorgehensweise ist. Topic Models enthalten fast immer einige Topics, die sich nicht mit Blick auf das Forschungsinteresse interpretieren lassen (Maier et al. 2018). Diese werden dann häufig von den weiteren Analysen ausgeschlossen. Die Analysen der interpretierbaren Topics sind davon nicht weiter beeinträchtigt, solange die nicht interpretierbaren Topics keine relevanten Informationen aus den interpretierbaren Topic entfernen. Wir könnten nach dieser Logik also auch das Modell mit \\(k = 60\\) zum Ausgangspunkt der Suche nach dem nützlichsten Modell machen und viele Topics ausschließen. Die Argumentation wäre dann, dass die verbliebenen Topics besonders klar herausstechen, da die nicht relevanten Charakteristika der Dokumente in den nicht relevanten Topics stecken. Aus diesem Grund habe ich als nächsten Schritt das Modell mit \\(k = 40\\) zum Vergleich herangezogen. Dabei bestätigt sich nach meiner Wahrnehmung die Vermutung, dass (etwas) mehr als 30 Topics eine geeignete Wahl sein könnten. 3.2.4 Schätzen und Vergleich von weiteren Modellen zwischen \\(k = 30\\) und \\(k = 40\\) Die Spezifikation und er quantitative Vergleich erfolgt nach demselben Prinzip wie zuvor. Der Code ist daher hier nicht dargestellt, kann aber in R/model_compare2.R gefunden werden. Wir vergleichen alle Modelle zwischen \\(k = 30\\) und \\(k = 40\\). Ein Blick auf die Skalierung der Achsen zeigt, dass die Modelle sich untereinander nur wenig unterscheiden. Die quantitativen Metriken helfen uns bei der Entscheidung zwischen diesen Modellen nicht weiter. Es folgt ein recht aufwändiger qualitativer Prozess, in dem wir entscheiden, ob die zusätzlichen Themen die Beschreibung der Dokumentinhalte substantiell verbessern. Als Ausgangspunkt nehmen wir das Modell mit 30 Topics. Daneben legen wir das Modell mit 31 Topics. Viele Topic-Labels sollten identisch oder sehr ähnlich sein. Unsere besondere Aufmerksamkeit gilt den Topics, deren typische Features sich verändert haben und die neue hinzu gekommen sind. Dazu können wir wieder {stminsights} oder den oben dargestellten Code (zu finden in R/model_qualihelper.R) nutzen. Ich habe für meine eigene Arbeit eine einfache Hilfsfunktion complete_labels() geschrieben, die ausgehend von Referenz-Topic-Labels für ein Modell die FREX-Features einer Liste von Topic Models vergleicht und die Labels weiter gibt, wenn sich mindestens eine vorgegebene Zahl FREX-Termen überschneiden. Ich stelle die Funktion im GitHub-Repository zur eigenverantwortlichen Verwendung zur Verfügung. Vorsicht: Die Funktion führt keine Checks durch. Ihr müsst selbst prüfen, ob die Referenz-Labels das richtige Format haben (tibble mit den Variablen topic und label) die Modelle in der Liste wirklich vergleichbar sind (auf vergleichbaren Daten basieren) Im Arbeitsverzeichnis keine Dateien mit der Bezeichnung “labels_k?.csv” vorhanden sind - diese werden kommentarlos überschrieben. Mit dieser Funktion könnt ihr die Labels, die ihr für die Modelle mit \\(k = 30\\) oder \\(k = 40\\) schon erstellt habt, bei einer Überschneidung von FREX-Features auf die weiteren Modelle übertragen. Das spart etwas Arbeit beim checken der einzelnen Modelle. Topics, die bei 10 berücksichtigen FREX-Features 8 oder mehr Überschneidungen haben, sollten eine sehr ähnliche inhaltliche Interpretation haben. Ich empfehle aber trotzdem, die jeweiligen Topics nach dem oben vorgestellten Vorgehen kurz zu prüfen, um eventuelle Veränderungen bemerken zu können. Eine Beschreibung des Vorgehens beim qualitativen Modellvergleich findet ihr als Video im LMS [kommt nach Fertigstellen des Textmaterials]. Ich habe das Modell mit \\(k = 37\\) Topics für die weiteren Analysen ausgewählt. Die Labels der Topics sind in R/data/labels_k37.csv gespeichert. Literatur "],
["modellinterpretation-und-darstellung.html", "Abschnitt 4 Modellinterpretation und -darstellung 4.1 Test der Modellqualität 4.2 Darstellung des Modells 4.3 Weitere Analysen: Prädiktoren der Topic-Prävalenz und Topic-Cluster", " Abschnitt 4 Modellinterpretation und -darstellung 4.1 Test der Modellqualität {oolong} ist ein recht neues Paket, das die Durchführung von standardisierten Tests der Topic-Qualität erleichtert. Ich habe es selbst bisher noch nicht in produktiven Projekten eingesetzt, werde es aber in zukünftigen Projekten tun. Infos zur technischen Umsetzung gibt es hier: https://github.com/chainsawriot/oolong/blob/master/overview_gh.md. Es werden zwei Tests angeboten: Der word intrusion test zeigt für jedes Topic die typischsten Features an und fügt zusätzlich ein Wort hinzu, dass nicht typisch für das Topic ist (intruder). Die Tester*innen müssen raten, welches Wort nicht zu den anderen passt. Je größer der Anteil der richtig erkannten intruder (hier als precision bezeichnet), desto besser lassen sich die Topics anhand der typischen Features interpretieren. Bei mehreren Tester*innen wird zusätzlich der aus der manuellen Inhaltsanalyse bekannte Koeffizient der Intercoder-Reliabilität Krippendorffs \\(\\alpha\\) ausgegeben. Der topic intrusion test zeigt eine Auswahl von Dokumenten aus dem Korpus. Dazu werden eine vorgegebene Zahl von Topics angezeigt, die in einem Dokument am wahrscheinlichsten enthalten sind. Ein weiteres Topic wird angezeigt, das in diesem Dokument nicht enthalten ist. Die Tester*innen müssen raten, welches Topic nicht zum Dokument passt. Als Ergebnis werden die topic log odds (TLO) berichtet. Sie quantifizieren die Wahrscheinlichkeit, dass das unpassende Topic gewählt wurde, korrigiert um die Wahrscheinlichkeit, dass das unpassende Topic einfach nur zufällig geraten wurde. Perfektes Erkennen der falschen Topics bei allen Dokumenten ergibt \\(TLO = 0\\). Diese Tests sollten von mehreren Personen durchgeführt werden. Im Idealfall werden die Modelle auch durch Personen getestet, die nicht am Projekt (oder zumindest nicht an der Modellierung) beteiligt waren, um die intersubjektive Nachvollziehbarkeit der Interpretationen zu testen. Aber schon formale Tests nur mit den Projektbeteiligten schlagen den heute üblichen Standard um Weiten. Bisher werden zur Validierung vor allem informelle Diskussionen eingesetzt (Maier et al. 2018) — so auch in unserer Beispielstudie. Der folgende Code zeigt das Erstellen, durchführen und Auswerten der Tests. Da die Tests interaktiv sind, wird er hier nicht ausgeführt. Ich zeige das Vorgehen in einem Video im LMS [kommt nach Fertigstellen des Textmaterials]. m37 = read_rds(&quot;R/data/model37.rds&quot;) out = read_rds(&quot;R/data/out.rds&quot;) # Erstellen eines Tests m37_oolong = create_oolong( input_model = m37, # Modell, das wir testen wollen input_corpus = out$meta$txt, # Korpus, auf dem Modell basiert; können wir aus &quot;out&quot; für stminsights nehmen use_frex_words = TRUE, # FREX-Features in beiden Tests nutzen n_top_terms = 5, # Zahl der korrekten Features im word intrusion test difficulty = 0.5, # Schwierigkeit des word intrusion tests; 0.5 = frexweight, das wir zur Interpretation genutzt haben bottom_terms_percentile = 0.4, # Definition der intruder words; hier: haben theta &lt; 0.4 n_topiclabel_words = 10, # Zahl der Features, die als Label im topic intrusion test angezeigt werden n_top_topics = 2, # Zahl der besten Topics, die für ein Dokument gezeigt werden exact_n = 5 # Zahl der Dokumente für topic intrusion test (alternativ frac für Anteil); in echtem Test mehr Dokumente codieren, hier nur 5, damit Demo nicht so lange dauert ) # Ausführen der Tests; Durchführen interaktiv in Viewer m37_oolong$do_word_intrusion_test() m37_oolong$do_topic_intrusion_test() # Beenden des Tests m37_oolong$lock() # Test-Ergebnisse m37_oolong_res = m37_oolong %&gt;% summarise_oolong() m37_oolong_res 4.2 Darstellung des Modells Eine Zusammenfassung der zentralen Ergebnisse eines Topic Models können wir mit einem Balkendiagramm leisten, dass auf der X-Achse die Prävalenz der Topics im gesamten Korpus zeigt. Auf der Y-Achse werden die Labels der Topics abgetragen. Um zusätzlich die wichtigsten Features, auf denen die Interpretation eines Topics basiert, zu vermitteln, können diese auf der rechten Seite neben den Balken dargestellt werden. Im folgenden Code-Snippet wird diese Abbildung erstellt: Für den Plot brauchen wir das Objekt mit dem geschätzten Modell (m37) und die Topic-Labels (m37_labels). Die Prävalenz der Topics in den einzelnen Dokumenten kann mit tidy(\"gamma\") in einen tidy data frame extrahiert werden. Der Mittelwert von “gamma” über die Dokumente ergibt den Anteil der Topics am gesamten Korpus, die Summe die Anzahl der Dokumente. Die typischen Features für die Topics, hier die FREX-Features, extrahieren wir mit labelTopics(). Wir fassen sie zu einer Tabelle zusammen, die jeweils alle Features für ein Topic in einer Zelle getrennt mit “,” enthält. Zuletzt fusionieren wir die Datensätze und erstellen die Abbildung mit ggplot(). Dieselbe Inhformation können wir auch als Tabelle darstellen. # Data m37 = read_rds(&quot;R/data/model37.rds&quot;) m37_labels = read_csv(&quot;R/data/labels_k37.csv&quot;) m37_labels = m37_labels %&gt;% mutate(label = str_c(topic, label, sep = &quot;: &quot;)) # Prävelenz der Topics m37_gamma = m37 %&gt;% tidy(&quot;gamma&quot;) %&gt;% group_by(topic) %&gt;% summarise(P = mean(gamma), n = sum(gamma)) # Frex Terms m37_frex = m37 %&gt;% labelTopics(n = 10) %&gt;% pluck(&quot;frex&quot;) %&gt;% as_tibble(.name_repair = ~ paste0(&quot;r&quot;, 1:10)) %&gt;% mutate(topic = 1:n()) %&gt;% gather(rank, frex_term, -topic) %&gt;% group_by(topic) %&gt;% summarise(frex_terms = paste0(frex_term, collapse = &quot;, &quot;)) # Plot m37_gamma %&gt;% left_join(m37_labels) %&gt;% left_join(m37_frex) %&gt;% mutate(label = reorder(label, P)) %&gt;% ggplot(aes(P, label, label = frex_terms)) + geom_bar(stat = &quot;identity&quot;) + scale_x_continuous(labels = scales::percent_format(accuracy = 1), expand = c(0, 0), limits = c(0, 0.1)) + geom_text(hjust = 0, nudge_x = 5e-04, size = 3) + labs(x = &quot;Anteil des Topics am gesamten Korpus&quot;, y = NULL) Figure 4.1: Zur vollen Ansicht in neuem Tab öffnen # Table m37_gamma %&gt;% left_join(m37_labels) %&gt;% left_join(m37_frex) %&gt;% select(label, P, frex_terms) %&gt;% mutate(P = paste0(round(P*100), &quot;%&quot;)) %&gt;% kable() label P frex_terms 1: Alter bei frühkindlicher Impfung 4% 3, monate, 2, monaten, 6, 1, 4, +, 5, fach 2: Reaktion auf Impfung (MMR) 2% sohn, später, reaktion, mmr, reagiert, MMR_IMPFUNG, fieber, infekt, tagen, kh 3: Zweifel und Unsicherheit 3% vielleicht, weiß, denke, lieber, frag, nochmal, sprechen, eventuell, interessieren, darüber 4: Familienkommunikation 3% familie, schwer, ehrlich, mann, alleine, oma, leben, verhalten, dinge, mutter 5: Entwicklung über die Zeit (in D) 2% mehr, deutschland, jahren, mittlerweile, weiss, jedenfalls, seit, tragen, anscheinend, fast 6: Fragen an Arzt und Hebamme 2% fragen, kinderarzt, glaube, stelle, allein, hebamme, anrufen, darf, setzen, braucht 7: Informationsquellen (insb. Hirte) 4% pro, impfen, entschieden, sinnvoll, lässt, stiko, lasse, empfehlen, buch, definitiv 8: Fieber 2% u, geben, kleinen, grad, schmerzen, gegeben, regelmäßig, hände, waschen, 39 9: Schutz von / Gefahr für andere/n Kinder, Impfpflicht 3% kinder, NICHT_GEIMPFT, &gt;, eltern, impfpflicht, krankheiten, ungeimpfte, ungeimpften, verantwortung, geimpfte 10: Verschiedene Kombi-Impfstoffe (bei Erwachsenen) 2% keuchhusten, geimpft, windpocken, tetanus, schutz, brauchst, anstecken, trotzdem, auffrischen, polio 11: Nebenwirkungen von Impfungen 3% nebenwirkungen, impfung, viren, nebenwirkung, erreger, impfstoff, körper, hpv, impft, mögliche 12: Großes Risiko, schwere gesundheitliche Konsequenzen 3% risiko, sicher, fälle, daran, passiert, folgen, gott, schäden, erkrankt, 100 13: Urlaub mit Baby, Umgang mit Baby 3% urlaub, warum, eigentlich, baby, verstehe, bevor, fliegen, kommt, niemanden, irgendwas 14: Kranke Kinder 3% kind, husten, krank, normale, schlimmer, ansteckend, krankenhaus, würdest, krankheit, schwester 15: Danke 3% danke, morgen, guten, gestern, antworten, beruhigt, heute, antwort, dank, schönen 16: Grippe-Impfung 4% grippe, lassen, grippeimpfung, GRIPPE_IMPFEN, nie, empfohlen, influenza, während, geraten, schwangerschaft 17: Meinungen und Informationen 3% meinung, besten, beste, seiten, informieren, sachen, wissen, thema, eigene, verschiedene 18: Spezifische Diskussion 1 2% rki, bereits, vermutlich, statt, bzgl, lesen, steht, usa, link, zurück 19: Schlafen und (nachts) trinken von Babies 3% schläft, flasche, trinkt, trinken, milch, minuten, bett, wasser, essen, brust 20: Beziehungsprobleme 2% nehmen, vertrauen, genommen, ernst, gesund, freund, klar, darm, funktioniert, böse 21: Masern, Viren, Nestschutz 2% antikörper, virus, nestschutz, erkrankung, entweder, angesteckt, personen, früher, infektion, masern 22: Impfschutz in der Schwangerschaft (insb. Röteln) 2% röteln, immun, ringelröteln, titer, mumps, immunität, ssw, impfpass, s, frauenarzt 23: Hilfe durch Arzt 3% hilft, arzt, schnell, helfen, eher, haut, kopf, sieht, schreien, kia 24: Frühe Kombi-Impfungen 3% rotaviren, pneumokokken, vertragen, b, erste, meningokokken, c, bisher, GUT_VERTRAGEN, zweite 25: Arbeiten und Berufsverbot 3% arbeiten, praxis, arbeite, arbeit, bv, hause, krippe, gehen, schule, kita 26: Ärzte und Patienten 2% ärzte, zumindest, gefühl, stimmt, patienten, schau, beraten, ggf, irgendwo, kinderärzte 27: Zeitliche Abstände 3% woche, wochen, zwei, letzte, nächste, drei, großen, kam, kleine, 8 28: Evidenzlage in der Impfdebatte 4% impfgegner, studien, pharmaindustrie, menschen, zusammenhang, impfschäden, leute, autismus, impfstoffe, artikel 29: Meta-Kommunikation 2% echt, sorry, wohl, bitte, bleibt, stellen, oh, nämlich, scheint, schreiben 30: Medizinische Tests 3% sagte, termin, meinte, blut, warten, gemacht, ärztin, negativ, untersuchung, klinik 31: Milde Impffolgen 3% tag, danach, nächsten, geschlafen, vorbei, ging, kurz, weinen, KEIN_FIEBER, geweint 32: Meta-Kommunikation 2 4% wirklich, finde, beitrag, gut, schlecht, sache, verstehen, art, te, schwierig 33: Wünsche und Hoffnungen 2% hoffe, bald, wünsche, gute, glück, leider, leid, 🍀, daumen, gibt’s 34: Grüße und Fragen 3% grüße, liebe, frage, 😉, erfahrungen, hey, holen, bisschen, jemand, hallo 35: FSME 2% jahr, fsme, tochter, zecken, damals, letztes, jahre, lag, flach, sommer 36: Emoji 2% 😂, 😊, 🙈, 😅, mama, schön, bauch, 🙄, genug, papa 37: Spezifische Diskussion 2 1% alter, meisten, tödlich, zudem, jedoch, davon, verzichten, mütter, möglich, natürlich Für den Anhang einer Arbeit, in der die Ergebnisse eines Topic Models berichtet werden, bietet sich eine ausführliche Dokumentation der Topics und der ihrer Interpretation zugrunde liegenden Features und Dokumente an. Damit wird die intersubjektive Nachvollziehbarkeit der qualitativen Interpretation gestärkt. Ein Beispiel für eine ausführliche Dokumentation findet sich hier. Die Dokumentation wurde größtenteils automatisch generiert. Nur die englischen Übersetzungen wurden aus einem manuell erstellten Word-Dokument eingelesen. Daten und Code zum Erstellen dieser Dokumentation sind hier und im assoziierten OSF-Repository. 4.3 Weitere Analysen: Prädiktoren der Topic-Prävalenz und Topic-Cluster Ein zentraler Vorteil von Structural Topic Models und ihrer Implementierung in stm ist die Möglichkeit, Zusammenhänge von Kovariaten mit den latenten Topics direkt zu schätzen. Damit wird die latente Struktur der Topics und die darin enthaltene Unsicherheit bei der Schätzung berücksichtigt. Wichtig: Prädiktoren sollten bereits in der Spezifikation des Topic Model (siehe Abschnitt 3.1) enthalten sein. Schätzung der Zusammenhänge mit anderen Prädiktoren zwar möglich, aber nicht ideal (Roberts, Stewart, and Tingley 2019). Hier betrachten wir eine typische Analyse: Wie haben sich die Prävalenzen der Topics über den Untersuchungszeitraum hinweg verändert? Das folgende Code-Snippet zeigt das Vorgehen bei der Analyse und mögliche Ergebnisdarstellungen: Zuerst schätzen wir das Modell mit \\(k = 37\\) noch einmal und spezifizieren dabei die Variable date_num (das Datum der Veröffentlichung eines Posts als numerische Variable in Tagen, 0 = aktuellster Post im Untersuchungszeitraum) als Kovariate Da wir nicht einfach nur einen linearen Trend schätzen wollen, schätzen wir den Zusammenhang als spline (~s(date_num)); siehe zu Details ?splines::bs. Allgemein gesprochen schätzen wir nicht-lineare Veränderungen in den Topic-Prävalenzen. Mit der Funktion estimateEffect() schätzen wir die Zusammenhänge der Topic-Prävalenzen mit der Kovariate, Vor der Tilde ~ geben wir an, für welche Topics wir die Zusammenhänge schätzen wollen. Hier schreiben wir 1:37, da wir sie für alle 37 Topics erhalten wollen. 2:5 würde z.B. nur die Zusammenhänge für die Topics 2 bis 5 schätzen. Nach der Tilde geben wir die Formel für die Kovariaten an. Hier geben wir, wie empfohlen, die Formel genau so an, wie wir sie beim Schätzen des Modells spezifiziert haben. Es wäre auch möglich, hier eine andere funktionale Form (z.B. nur ~ date_num für einen linearen Trend) oder andere Kovariaten aufzunehmen. Mit der Funktion get_effects() extrahieren wir einen tidy data frame, auf dessen Basis wir verschiedene Visualisierungen erstellen können. Hier fügen wir auch die Labels der Topics hinzu und formatieren die Datumsvariable wieder als Datum. Die beiden Abbildungen zeigen die Entwicklung der Topics über den Untersuchungszeitraum. # Data m37_labels = read_csv(&quot;R/data/labels_k37.csv&quot;) m37_labels = m37_labels %&gt;% mutate(label = str_c(topic, label, sep = &quot;: &quot;)) out = read_rds(&quot;R/data/out.rds&quot;) # Schätzen des Modells mit Kovariate if (file.exists(&quot;R/data/m37_we.rds&quot;)) { # Laden des Modells mit Kovariate m37_we = read_rds(&quot;R/data/m37_we.rds&quot;) } else { # Schätzen des Modells mit Kovariate m37_we = stm(documents = out$documents, vocab = out$vocab, data = out$meta, prevalence = ~s(date_num), # In dieser Zeile spezifizieren wir die Kovariate init.type = &quot;Spectral&quot;, K = 37, verbose = FALSE) saveRDS(m37_we, &quot;R/data/m37_we.rds&quot;) } # Effect Objekt if (file.exists(&quot;R/data/m37_effects.rds&quot;)) { # Laden der Effekt-Schätzung m37_effects = read_rds(&quot;R/data/m37_effects.rds&quot;) } else { # Effekt-Schätzung m37_effects = m37_we %&gt;% estimateEffect(1:37 ~ s(date_num), stmobj = ., metadata = out$meta) saveRDS(m37_effects, &quot;R/data/m37_effects.rds&quot;) } # Daten für Plot eff_plot_data = m37_effects %&gt;% get_effects(variable = &quot;date_num&quot;, type = &quot;continuous&quot;) %&gt;% mutate(topic = as.integer(as.character(topic)), date = as_date(value + max(as.numeric(out$meta$postdate)))) %&gt;% left_join(m37_labels) %&gt;% mutate(label = reorder(label, topic, mean)) # Plot: Alle Topics über die Zeit plt1 = eff_plot_data %&gt;% ggplot(aes(date, proportion, ymin = lower, ymax = upper)) + geom_ribbon(alpha = 0.5) + geom_line() + facet_wrap(&quot;label&quot;, ncol = 2) + scale_y_continuous(labels = scales::percent_format(), limits = c(0, 0.08)) # Plot: Grippe und Evidenz plt2 = eff_plot_data %&gt;% filter(topic %in% c(16, 28)) %&gt;% ggplot(aes(date, proportion, ymin = lower, ymax = upper, group = label)) + geom_ribbon(alpha = 0.5) + geom_line(aes(color = label)) + scale_y_continuous(labels = scales::percent_format(), limits = c(0, 0.08)) Figure 4.2: Zur vollen Ansicht in neuem Tab öffnen Figure 4.3: Zur vollen Ansicht in neuem Tab öffnen Analysen von mittleren oder großen Korpora erfordern Topic Models mit einer recht großen Anzahl von Topics (siehe Abschnitt 3). Für ein besseres Verständnis der Inhalte im Korpus und für eine einfachere Ergebnispräsentation kann es manchmal sinnvoll sein, die Topics weiter zusammenzufassen. Eine Möglichkeit ist eine Betrachtung, welche Topics häufig gemeinsam in Dokumenten vorkommen. Es liegt nahe, dass solche Topics etwas miteinander zu tun haben. Eine hierarchische Cluster-Analyse des gemeinsamen Auftretens von Topics in Dokumenten erlaubt einen einfachen empirischen Zugang. Der folgende Code zeigt, wie wir eine solche Analyse für das Modell mit 37 Topics durchführen können. Wir erstellen eine Distanzmatrix der Topics m37_dist aus der Dokument-Topic-Matrix m37$theta. Als Distanzmaß wählen wir die Hellinger’s Distance, die im Paket {textmineR} implementiert ist. Die Cluster-Analyse führen wir mit der Funktion hclust() und dem Ward-Algorithmus durch. Aus dem Dendogramm leiten wir eine für unsere Zwecke nützliche Zahl von Clustern ab. In diesem Fall habe ich mich für 7 Cluster entschieden. Die Abbildung zur Prävalenz der Topics lässt sich nun nach der Zugehörigkeit der Topics zu den Clustern darstellen. # Data m37 = read_rds(&quot;R/data/model37.rds&quot;) m37_labels = read_csv(&quot;R/data/labels_k37.csv&quot;) m37_labels = m37_labels %&gt;% mutate(label = str_c(topic, label, sep = &quot;: &quot;)) # Hierarchische CLusteranalyse des gemeinsamen Auftretens von Topics in Dokumenten m37_dist = textmineR::CalcHellingerDist(m37$theta, by_rows = FALSE) clusters = hclust(as.dist(m37_dist), &quot;ward.D2&quot;) clusters$labels = m37_labels$label # Dendogramm ggdendro::ggdendrogram(clusters, rotate = TRUE) + labs(title = NULL) + scale_y_continuous(limits = c(0,1), expand = c(0,0), name = &quot;Distanz&quot;) + theme(axis.text.y = element_text(size = 10, colour = &quot;black&quot;), axis.title = element_text(size = 14, face = &quot;bold&quot;)) Figure 4.4: Zur vollen Ansicht in neuem Tab öffnen # Plot mit 7 Topic-Clustern m37_gamma %&gt;% mutate(grp = case_when(topic %in% c(21, 10, 11, 7, 16) ~ 1, topic %in% c(27, 1, 2, 35, 24) ~ 2, topic %in% c(31, 8, 19) ~ 3, topic %in% c(34, 15, 30, 23, 6, 33, 36) ~ 4, topic %in% c(28, 17, 12, 9, 18) ~ 5, topic %in% c(32, 29, 37, 5, 3, 13, 14, 26, 20, 4) ~ 6, topic %in% c(25, 22) ~ 7)) %&gt;% left_join(m37_labels) %&gt;% left_join(m37_frex) %&gt;% mutate(label = reorder(label, P)) %&gt;% ggplot(aes(P, label, label = frex_terms, fill = factor(grp))) + geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) + scale_x_continuous(labels = scales::percent_format(accuracy = 1), expand = c(0, 0), limits = c(0, 0.1)) + geom_text(hjust = 0, nudge_x = 5e-04, size = 3) + labs(x = &quot;Anteil des Topics am gesamten Korpus&quot;, y = NULL) + facet_grid(grp ~ ., scales = &quot;free_y&quot;, space = &quot;free_y&quot;) Figure 4.5: Zur vollen Ansicht in neuem Tab öffnen Literatur "],
["literatur.html", "Literatur", " Literatur "]
]
