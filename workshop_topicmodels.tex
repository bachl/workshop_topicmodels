% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Mini-Workshop (Structural) Topic Models},
  pdfauthor={Marko Bachl},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Mini-Workshop (Structural) Topic Models}
\author{Marko Bachl}
\date{Sommersemester 2020 \textbar{} IJK Hannover}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{uxfcberblick}{%
\chapter{Ãœberblick}\label{uxfcberblick}}

\hypertarget{inhalt-des-virtuellen-mini-workshops}{%
\section{Inhalt des virtuellen Mini-Workshops}\label{inhalt-des-virtuellen-mini-workshops}}

\begin{itemize}
\tightlist
\item
  In diesem Mini-Workshop erlÃ¤utere ich das praktische Vorgehen einer Datenanalyse mit \emph{Structural Topic Models}. Wir behandeln die folgenden Schritte im Analyseprozess:

  \begin{itemize}
  \tightlist
  \item
    Modellspezifikation
  \item
    Modellvergleich zur Auswahl eines geeigneten Modells
  \item
    Interpretation der Topics im finalen Modell
  \item
    Darstellung der Ergebnisse
  \item
    Weitere Analysen

    \begin{itemize}
    \tightlist
    \item
      Identifikation verwandter Themen
    \item
      ZusammenhÃ¤nge der ThemenprÃ¤valenz mit Kovariaten.
    \end{itemize}
  \end{itemize}
\item
  Wir verwenden das Paket \texttt{\{stm\}} \citep{robertsStmPackageStructural2019} zum SchÃ¤tzen von Topic Models. FÃ¼r die Variante der \emph{Structural} Topic Models und die Implementation in diesem Paket sprechen \emph{fÃ¼r mich} die folgenden GrÃ¼nde

  \begin{itemize}
  \tightlist
  \item
    Gute Integration mit \emph{R} und Paketen, die ich fÃ¼r die Arbeit mit Text-Daten verwende (insbesondere \texttt{\{quanteda\}} und \texttt{\{tidytext\}})
  \item
    Gute ergÃ¤nzende Pakete zur Arbeit mit den Modellen (insbesondere \texttt{\{stminsights\}})
  \item
    Vergleichsweise schnelle ModellschÃ¤tzung auch mit groÃŸen DatensÃ¤tzen
  \item
    Direktes SchÃ¤tzen von ZusammenhÃ¤ngen von Topics mit Kovariaten
  \item
    Initialisieren der ModellschÃ¤tzung mit dem Spectral Algorithmus
  \item
    Recht weit verbreitet in einem Feld, in dem ich viel lese (Politische Kommunkation nach einem weitem VerstÃ¤ndnis)
  \end{itemize}
\item
  Die Darstellung basiert auf einer Analyse, die ich gemeinsam mit Elena Link durchgefÃ¼hrt habe. Wir untersuchten, wie das Thema Impfen in Online-Foren fÃ¼r Eltern diskutiert wurde. Wir verwenden aber nur einen \emph{nicht reprÃ¤sentativen} Ausschnitt aus dem Material, um die notwendige Rechenleistung und -zeit zu verringern.

  \begin{itemize}
  \tightlist
  \item
    Einen Preprint zur Analyse kÃ¶nnt ihr hier lesen: \href{https://osf.io/ad9h7/}{Vaccine-related Discussions in Online Communities for Parents. A Quantitative Overview}.
  \item
    Die Dokumentation zur Studie ist hier verfÃ¼gbar: \url{https://bachl.github.io/vaccine_discussions/}. Daten und Analyse-Skripts gibt es im \href{https://osf.io/twx38/}{OSF}. Dort werden auch die Datenerhebung mittels Web-Scraping und die Datenaufbereitung erlÃ¤utert. Diese Inhalte sind \emph{nicht} Teil dieses Workshops. Wenn ihr Fragen dazu habt, dÃ¼rft ihr sie natÃ¼rlich stellen.
  \end{itemize}
\end{itemize}

\hypertarget{welche-inhalte-wir-nicht-behandeln}{%
\section{\texorpdfstring{Welche Inhalte wir \emph{nicht} behandeln}{Welche Inhalte wir nicht behandeln}}\label{welche-inhalte-wir-nicht-behandeln}}

\begin{itemize}
\item
  Auch wenn das im direkten Vergleich mit dem Parallel-Angebot zu \href{https://bachl.github.io/workshop_panel/}{Panel Data Analysis} (meine AusfÃ¼hrlichkeit dort sind ein Grund fÃ¼r die spÃ¤tere Lieferung dieser Materialien) enttÃ¤uschend sein mag: Die Inhalte in diesem Mini-Workshop entsprechen in ihrem Umfang wirklich nur dem, was ich zu Beginn des Digital-Semesters geplant und angekÃ¼ndigt hatte. Der Mini-Workshop ersetzt keine tiefer gehende Einarbeitung in die Methode, sondern ist als ein Einstieg zu verstehen.
\item
  Wir behandeln hier keine theoretischen, statistischen oder auf die Software-Implementierung der ModellschÃ¤tzung bezogenen Fragen. Die Grundlagen dazu kÃ¶nnen aus den Texten im LMS entnommen werden \citep{maierApplyingLDATopic2018, robertsStmPackageStructural2019}.
\item
  Es gibt neben \texttt{\{stm\}} viele andere Implementationen in \emph{R} und ihn anderer Software. GefÃ¼hlt gibt es alle 6 Monate eine neue Variante von Topic Models, alle 3 Monate eine neue Implementierung und jeden Monat ein Paket mit zusÃ¤tzlichen Tools fÃ¼r die Arbeit mit Topic Models. Meine Entscheidung fÃ¼r \texttt{\{stm\}} ist keine informierte Entscheidung gegen andere Varianten, Implementierungen und Tools. Dieser Workshop ist keine Aufforderung, ausschlieÃŸlich \texttt{\{stm\}} zu nutzen. Informiert euch gegebenenfalls selbst Ã¼ber Software-LÃ¶sungen, die fÃ¼r eure BedÃ¼rfnisse geeignet sind.
\item
  Dieser Mini-Workshop ist kein \emph{R}-Tutorial. Wenn ihr Interesse habt, \emph{R}-Kenntnisse zu erwerben und zu vertiefen, empfehle ich \href{https://r4ds.had.co.nz/}{R4DS}.
\item
  Dieser Mini-Workshop ist keine allgemeine EinfÃ¼hrung in die computergestÃ¼tzte Inhaltsanalyse. Wenn ihr allgemein mit \emph{R} arbeiten mÃ¶chtet, empfehle ich zu diesem Thema die \href{http://inhaltsanalyse-mit-r.de/}{EinfÃ¼hrung von Cornelius Puschmann}.
\end{itemize}

\hypertarget{aufbau-des-workshops}{%
\section{Aufbau des Workshops}\label{aufbau-des-workshops}}

\begin{itemize}
\tightlist
\item
  Inhaltlicher Aufbau: Siehe Kapitel-Gliederung
\end{itemize}

\hypertarget{material}{%
\subsection*{Material}\label{material}}
\addcontentsline{toc}{subsection}{Material}

\begin{itemize}
\item
  Dieses Dokument + R Skripte: (Hoffentlich) mehr oder weniger selbsterklÃ¤rendes Material

  \begin{itemize}
  \tightlist
  \item
    Kuratierte Form ist dieses HTML-Dokument
  \item
    Es gibt auch ein PDF, das ich aber nicht formatiert habe
  \end{itemize}
\item
  Daten: Ein Ausschnitt auf den Daten der oben genannten Beispielstudie. Eine genauere Beschreibung folgt im nÃ¤chsten Abschnitt.
\item
  Screencast: Zu einigen Analyseschritten stelle ich Screencasts zur VerfÃ¼gung. Diese sind grÃ¶ÃŸtenteils ergÃ¤nzend gedacht. Bis auf wenige Ausnahmen sollte das schriftliche Material selbsterklÃ¤rend sein.
\item
  Ãœbungen: Zu einigen Analysen gibt es Ãœbungsaufgaben.

  \begin{itemize}
  \tightlist
  \item
    XXX
  \end{itemize}
\end{itemize}

\hypertarget{pakete}{%
\subsection*{Pakete}\label{pakete}}
\addcontentsline{toc}{subsection}{Pakete}

Wir verwenden die folgenden Pakete

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{require}\NormalTok{(}\StringTok{"pacman"}\NormalTok{)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"pacman"}\NormalTok{)}
\NormalTok{pacman}\OperatorTok{::}\KeywordTok{p_load}\NormalTok{(tidyverse, stm, stminsights, tidytext, quanteda, lubridate, knitr, }
\NormalTok{    tictoc, furrr)}
\KeywordTok{theme_set}\NormalTok{(}\KeywordTok{theme_bw}\NormalTok{())  }\CommentTok{# ggplot theme}

\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{package =} \KeywordTok{c}\NormalTok{(}\StringTok{"R"}\NormalTok{, }\KeywordTok{sort}\NormalTok{(pacman}\OperatorTok{::}\KeywordTok{p_loaded}\NormalTok{()))) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{version =} \KeywordTok{map_chr}\NormalTok{(package, }
    \OperatorTok{~}\KeywordTok{as.character}\NormalTok{(pacman}\OperatorTok{::}\KeywordTok{p_version}\NormalTok{(}\DataTypeTok{package =}\NormalTok{ .x)))) }\OperatorTok{%>%}\StringTok{ }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l}
\hline
package & version\\
\hline
R & 3.6.2\\
\hline
dplyr & 0.8.4\\
\hline
forcats & 0.4.0\\
\hline
furrr & 0.1.0\\
\hline
future & 1.16.0\\
\hline
ggplot2 & 3.3.1\\
\hline
knitr & 1.28\\
\hline
lubridate & 1.7.4\\
\hline
pacman & 0.5.1\\
\hline
purrr & 0.3.3\\
\hline
quanteda & 2.0.0\\
\hline
readr & 1.3.1\\
\hline
stm & 1.3.5\\
\hline
stminsights & 0.4.0\\
\hline
stringr & 1.4.0\\
\hline
tibble & 2.1.3\\
\hline
tictoc & 1.0\\
\hline
tidyr & 1.0.2\\
\hline
tidytext & 0.2.3\\
\hline
tidyverse & 1.3.0\\
\hline
\end{tabular}

\hypertarget{beispiel-daten-und-aufbereitung}{%
\chapter{Beispiel-Daten und Aufbereitung}\label{beispiel-daten-und-aufbereitung}}

\hypertarget{laden-der-daten-und-uxfcbersicht}{%
\section{Laden der Daten und Ãœbersicht}\label{laden-der-daten-und-uxfcbersicht}}

\begin{itemize}
\tightlist
\item
  Wir verwenden einen Ausschnitt der Daten aus der Beispielstudie. Konkret handelt es sich um Posts mit dem Suchwort \emph{impf}, die zwischen dem 1. Mai 2016 und dem 8. Juli 2019 im Elternforum \href{https://www.urbia.de/forum}{Urbia} verÃ¶ffentlicht wurden. Ausgeschlossen wurden unter anderem

  \begin{itemize}
  \tightlist
  \item
    sehr kurze Posts (weniger als 19 WÃ¶rter)
  \item
    Posts mit dem Wort \emph{schimpf}
  \item
    Posts zur Impfung von Haustieren (nach einem kurzen DiktionÃ¤r)
  \end{itemize}
\item
  Die Dokumentation zur Studie gibt weitere Informationen zur Erhebung und Bereinigung der Rohdaten.
\item
  Diese Daten kÃ¶nnen aus Copyright- und Privacy-GrÃ¼nden nicht auf GitHub verÃ¶ffentlicht werden. Ich habe Sie daher im LMS hochgeladen. Bitte ladet die ZIP-Datei herunter.

  \begin{itemize}
  \tightlist
  \item
    Wenn ihr sie mit dem Code aus dem Repository integrieren wollt, mÃ¼sst ihr sie in den Ordner ``data'' unter ``R'' entpacken.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Laden der Daten}
\NormalTok{d =}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"R/data/example_data.rds"}\NormalTok{)}
\NormalTok{d }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{print}\NormalTok{(}\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 12,369 x 5
##   post                         author   postdate      wc thread_title           
##   <chr>                        <chr>    <date>     <int> <chr>                  
## 1 Wenn Impfungen zu TodesfÃ¤ll~ zwerg-b~ 2018-04-06    26 HPV-Impfung            
## 2 Hallo Moni Danke fÃ¼r deine ~ Inaktiv  2017-06-03    21 Warum so oft Scheidenp~
## 3 Hallo ja sind glaube ich dr~ danerl   2017-06-05    42 Warum so oft Scheidenp~
## 4 Guten Morgen, gibt es hier ~ butterf~ 2017-05-14   133 Impfung Deutschland/Ã–s~
## 5 In Ã–sterreich wird im 3., 5~ butterf~ 2017-05-15    68 Impfung Deutschland/Ã–s~
## # ... with 1.236e+04 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{ym =} \KeywordTok{round_date}\NormalTok{(postdate, }\StringTok{"month"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(ym) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(ym, n)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{workshop_topicmodels_files/figure-latex/read-data-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pull}\NormalTok{(}\StringTok{"wc"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##      20      37      60      85     101    2493
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Der Datensatz besteht aus 12,369 Posts.

  \begin{itemize}
  \tightlist
  \item
    Die Variable \texttt{post} enthÃ¤lt den vollen Text des Posts.
  \item
    Die Variable \texttt{author} enthÃ¤lt den Accountnamen, von dem der Post abgegeben wurde.
  \item
    Die Variable \texttt{date} enthÃ¤lt den Tag der VerÃ¶ffentlichung.
  \item
    Die Variable \texttt{wc} enthÃ¤lt die Zahl der WÃ¶rter des Posts.
  \item
    Die Variable \texttt{thread\_title} enthÃ¤lt den Titel des Diskussions-Threads.
  \end{itemize}
\item
  Pro Monat sind zwischen ca. 120 und 1.000 Posts in unserer Stichprobe.
\item
  Typische Posts haben einen Umfang von zwischen 40 und 100 WÃ¶rtern (Zur Erinnerung: Sehr kurze Post wurden bereits ausgeschlossen).
\end{itemize}

\hypertarget{aufbereitung-fuxfcr-das-schuxe4tzen-der-topic-models}{%
\section{Aufbereitung fÃ¼r das SchÃ¤tzen der Topic Models}\label{aufbereitung-fuxfcr-das-schuxe4tzen-der-topic-models}}

\begin{itemize}
\tightlist
\item
  GrundsÃ¤tzlich gilt: Die verschiedenen Schritte bei der Aufbereitung des Text-Korpus kann die Ergebnisse wesentlich beeinflussen \citep{dennyTextPreprocessingUnsupervised2018, maierApplyingLDATopic2018}. Aber ist es hÃ¤ufig sehr schwierig, theoretisch informierte Entscheidungen zu treffen, da

  \begin{itemize}
  \tightlist
  \item
    unsere Theorien fast immer zu vage sind, um etwas Ã¼ber konkrete, manifeste Eigenschaften der Texte auszusagen
  \item
    es schwer ist, die Folge einer Entscheidung fÃ¼r das technische SchÃ¤tzen der Modelle und fÃ¼r die substanzielle Interpretation der Ergebnisse vorherzusagen,
  \item
    Entscheidungen \emph{post hoc} auf Basis der Ergebnisse wissenschaftstheoretisch und -praktisch problematisch sein kÃ¶nnen (\emph{overfitting}, \emph{harking} bzw. \emph{hindsight bias}, etc.).
  \end{itemize}
\item
  In der zugrunde liegenden Studie habe ich versucht, diese Entscheidungen \emph{a priori} zu treffen. Die Entscheidungen basieren aber zugegebenermaÃŸen mehr auf vagen Vermutungen und fÃ¼r mich plausiblen und pragmatischen Ãœberlegungen als auf einer konsistenten Theorie.

  \begin{itemize}
  \tightlist
  \item
    Entfernen von StoppwÃ¶rtern: StoppwÃ¶rter sind WÃ¶rter, die in einer Sprache hÃ¤ufig vorkommen und nicht wesentlich zur Bedeutung eines Texts beitragen. Hier habe ich auf Basis der deutschen Liste im Paket \texttt{\{stopwords\}} und der WorthÃ¤ufigkeiten im Korpus eine Liste erstellt. Durch das \emph{Pruning} der Dokument-Feature-Matrix (siehe unten) ist die Auswahl der StoppwÃ¶rter aber weniger entscheidend, da WÃ¶rter, die in sehr vielen Texten des Korpus vorkommen, ohnehin entfernt werden.
  \item
    ZusÃ¤tzliche BerÃ¼cksichtigung von Bi- und Tri-Grammen: Ich habe die Kombinationen von zwei oder drei WÃ¶rtern, die hÃ¤ufig im Korpus vorkamen, daraufhin gesichtet, ob sie fÃ¼r das Thema Impfen und gesundheitsrelevante Diskussionen zusÃ¤tzliche Informationen enthalten, die jedes einzelne Wort alleine nicht enthÃ¤lt. Diese Kombinationen wurden als zusÃ¤tzliche Features aufgenommen.
  \item
    Der Argumentation und den empirischen Ergebnissen von \citet{schofieldComparingApplesApple2016} (deren Aufsatz Ã¼brigens einen groÃŸartigen Titel hat, groÃŸer NLP Nerd Humor) folgend habe ich auf Stemming oder Lemmatisierung verzichtet. In der Tat zeigt sich, dass WÃ¶rter mit dem gleichen Wortstamm, wie von \citet{schofieldComparingApplesApple2016} beschrieben, hÃ¤ufig im selben Topic landen.
  \item
    Ãœblichen Standards \citep[z.B.][]{maierApplyingLDATopic2018} folgend habe ich alle WÃ¶rter in Kleinschreibung umgewandelt, Satzzeichen entfernt und URL entfernt. Zahlen habe ich beibehalten, da sie (wie die Ergebnisse auch zeigen) typische Merkmale bestimmter Perspektiven auf das Thema Impfen sind.
  \item
    Da wir auch an der VerÃ¤nderung der Topic-HÃ¤ufigkeiten Ã¼ber die Zeit interessiert sind, wird die Variable mit dem Erscheinungstags des Posts in eine numerische Variable umgewandelt. Sie ist so skaliert, dass der aktuellste Post den Wert 0 hat. Diese Variable kÃ¶nnen wir dann als PrÃ¤diktor beim SchÃ¤tzen des \emph{Structural Topic Model} berÃ¼cksichtigen.
  \item
    Unter \emph{Pruning} versteht man das Entfernen von Features, die entweder in sehr weniger oder in sehr vielen Dokumenten vorkommen. Dadurch kÃ¶nnen die GrÃ¶ÃŸe des Datensatzes und in der Folge die zum SchÃ¤tzen der Modelle nÃ¶tigen Ressourcen wesentlich reduziert werden. Inhaltlich sollte das Entfernen dieser Features wenig Ã¤ndern: Features, die in sehr vielen Dokumenten vorkommen, tragen nicht zur Differenzierung zwischen den Dokumenten bei. Features, die nur in sehr wenigen Dokumenten vorkommen, tragen nicht zur Definition von Topics bei, da diese durch das regelmÃ¤ÃŸige \emph{gemeinsame} Vorkommen in Dokumenten identifiziert werden. Siehe ausfÃ¼hrlich \citet{maierApplyingLDATopic2018}.
  \end{itemize}
\item
  Die Vorbereitung des Korpus und der Dokument-Feature-Matrix erfolgte mit Funktionen aus \texttt{\{quanteda\}}.

  \begin{itemize}
  \tightlist
  \item
    Mit der Funktion \texttt{corpus()} wird der Datensatz in einen Text-Korpus umgewandelt. In diesem Zuge wird auch die numerische Datums-Variable erstellt. Die Variable mit dem Text des Posts duplizieren wir, damit sie zusÃ¤tzlich als Meta-Datum fÃ¼r jeden Text gespeichert wird. Das wird spÃ¤ter hilfreich sein, wenn wir die Ergebnisse einer ModellschÃ¤tzung explorieren.
  \item
    \texttt{custom\_stopwords} und \texttt{relevant\_ngrams} zeigen die StoppwÃ¶rter und Wortkombinationen, die ausgeschlossen bzw. einbezogen werden. Letztere werden mit der Funktion \texttt{dictionary()} aus \texttt{\{quanteda\}} erstellt.
  \item
    Mit der Funktion \texttt{dfm()} wird der Korpus in eine Dokument-Feature-Matrix umgewandelt. Dabei werden die Standard-Schritte der Textaufbereitung durchgefÃ¼hrt. Sie besteht aus 12,369 Posts in den Zeilen und 41,385 Features in den Spalten. In jeder Zelle ist angegeben, wie hÃ¤ufig ein Feature in einem Dokument vorkommt.
  \item
    Mit der Funktion \texttt{dfm\_trim()} wird das Pruning durchgefÃ¼hrt. Dabei werden alle Features, die in weniger als 0.5\% oder mehr als 99\% der Posts vorkommen, entfernt. Nach dem Pruning enthÃ¤lt die Matrix nur noch 1,150 Features.
  \item
    Zuletzt muss die Matrix in das von \texttt{stm()} benÃ¶tigte Format konvertiert werden. Dabei werden zwei Posts gelÃ¶scht, die nach der Bereinigung kein einziges Feature mehr enthalten. Wichtig fÃ¼r den Bericht der Fallzahl in einer Publikation!
  \end{itemize}
\item
  Am Ende seht ihr eine einfache Beschreibung der hÃ¤ufigsten Features im Korpus als Tabelle und Wordcloud.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Erstellen des Korpus}
\NormalTok{crps =}\StringTok{ }\NormalTok{d }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{txt =}\NormalTok{ post, }\CommentTok{# Duplizieren des Post-Texts fÃ¼r Meta-Daten}
         \DataTypeTok{date_num =} \KeywordTok{as.numeric}\NormalTok{(postdate) }\OperatorTok{-}\StringTok{ }\KeywordTok{max}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(postdate))) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Numerische Datumsvariable}
\StringTok{  }\KeywordTok{corpus}\NormalTok{(}\DataTypeTok{text_field =} \StringTok{"post"}\NormalTok{) }\CommentTok{# Erstellen des Korpus}
\NormalTok{crps }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summary}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Corpus consisting of 12369 documents, showing 1 document:
## 
##   Text Types Tokens Sentences       author   postdate wc thread_title
##  text1    27     29         2 zwerg-bayern 2018-04-06 26  HPV-Impfung
##                                                                                                                                                                                                  txt
##  Wenn Impfungen zu TodesfÃ¤lle oder starken Nebenwirkungen in einzelnen FÃ¤llen fÃ¼hren, spricht dann fÃ¼r ImmungschwÃ¤che oder schlummernder Krankheit. Ein gesunder Mensch stirbt nicht an der Impfung.
##  date_num
##      -457
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# StoppwÃ¶rter}
\NormalTok{custom_stopwords =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"ab"}\NormalTok{, }\StringTok{"aber"}\NormalTok{, }\StringTok{"ach"}\NormalTok{, }\StringTok{"all"}\NormalTok{, }\StringTok{"alle"}\NormalTok{, }\StringTok{"allem"}\NormalTok{, }\StringTok{"allen"}\NormalTok{, }\StringTok{"aller"}\NormalTok{, }\StringTok{"alles"}\NormalTok{, }\StringTok{"als"}\NormalTok{, }\StringTok{"also"}\NormalTok{, }\StringTok{"am"}\NormalTok{, }\StringTok{"an"}\NormalTok{, }\StringTok{"andere"}\NormalTok{, }\StringTok{"anderen"}\NormalTok{, }\StringTok{"anderes"}\NormalTok{, }\StringTok{"anders"}\NormalTok{, }\StringTok{"auch"}\NormalTok{, }\StringTok{"auf"}\NormalTok{, }\StringTok{"aufs"}\NormalTok{, }\StringTok{"aus"}\NormalTok{, }\StringTok{"bei"}\NormalTok{, }\StringTok{"beim"}\NormalTok{, }\StringTok{"bin"}\NormalTok{, }\StringTok{"bis"}\NormalTok{, }\StringTok{"bist"}\NormalTok{, }\StringTok{"bzw"}\NormalTok{, }\StringTok{"da"}\NormalTok{, }\StringTok{"dabei"}\NormalTok{, }\StringTok{"dadurch"}\NormalTok{, }\StringTok{"daher"}\NormalTok{, }\StringTok{"dahin"}\NormalTok{, }\StringTok{"damit"}\NormalTok{, }\StringTok{"dann"}\NormalTok{, }\StringTok{"das"}\NormalTok{, }\StringTok{"dass"}\NormalTok{, }\StringTok{"daÃŸ"}\NormalTok{, }\StringTok{"dazu"}\NormalTok{, }\StringTok{"dein"}\NormalTok{, }\StringTok{"deine"}\NormalTok{, }\StringTok{"deinem"}\NormalTok{, }\StringTok{"deinen"}\NormalTok{, }\StringTok{"deiner"}\NormalTok{, }\StringTok{"dem"}\NormalTok{, }\StringTok{"den"}\NormalTok{, }\StringTok{"denen"}\NormalTok{, }\StringTok{"denn"}\NormalTok{, }\StringTok{"dennoch"}\NormalTok{, }\StringTok{"der"}\NormalTok{, }\StringTok{"deren"}\NormalTok{, }\StringTok{"des"}\NormalTok{, }\StringTok{"deshalb"}\NormalTok{, }\StringTok{"deswegen"}\NormalTok{, }\StringTok{"dich"}\NormalTok{, }\StringTok{"die"}\NormalTok{, }\StringTok{"dies"}\NormalTok{, }\StringTok{"diese"}\NormalTok{, }\StringTok{"diesem"}\NormalTok{, }\StringTok{"diesen"}\NormalTok{, }\StringTok{"dieser"}\NormalTok{, }\StringTok{"dieses"}\NormalTok{, }\StringTok{"dir"}\NormalTok{, }\StringTok{"doch"}\NormalTok{, }\StringTok{"dort"}\NormalTok{, }\StringTok{"dran"}\NormalTok{, }\StringTok{"drauf"}\NormalTok{, }\StringTok{"drin"}\NormalTok{, }\StringTok{"drÃ¼ber", "}\NormalTok{du}\StringTok{", "}\NormalTok{durch}\StringTok{", "}\NormalTok{durchaus}\StringTok{", "}\NormalTok{eh}\StringTok{", "}\NormalTok{ein}\StringTok{", "}\NormalTok{eine}\StringTok{", "}\NormalTok{einem}\StringTok{", "}\NormalTok{einen}\StringTok{", "}\NormalTok{einer}\StringTok{", "}\NormalTok{eines}\StringTok{", "}\NormalTok{einige}\StringTok{", "}\NormalTok{einigen}\StringTok{", "}\NormalTok{einiges}\StringTok{", "}\NormalTok{einmal}\StringTok{", "}\NormalTok{er}\StringTok{", "}\NormalTok{es}\StringTok{", "}\NormalTok{etc}\StringTok{", "}\NormalTok{etwas}\StringTok{", "}\NormalTok{euch}\StringTok{", "}\NormalTok{euer}\StringTok{", "}\NormalTok{eure}\StringTok{", "}\NormalTok{euren}\StringTok{", "}\NormalTok{fÃ¼r", }\StringTok{"fÃ¼rs", "}\NormalTok{gegen}\StringTok{", "}\NormalTok{gehabt}\StringTok{", "}\NormalTok{getan}\StringTok{", "}\NormalTok{gewesen}\StringTok{", "}\NormalTok{geworden}\StringTok{", "}\NormalTok{hab}\StringTok{", "}\NormalTok{habe}\StringTok{", "}\NormalTok{haben}\StringTok{", "}\NormalTok{habt}\StringTok{", "}\NormalTok{halt}\StringTok{", "}\NormalTok{hast}\StringTok{", "}\NormalTok{hat}\StringTok{", "}\NormalTok{hatte}\StringTok{", "}\NormalTok{hÃ¤tte}\StringTok{", "}\NormalTok{hatten}\StringTok{", "}\NormalTok{hÃ¤tten}\StringTok{", "}\NormalTok{her}\StringTok{", "}\NormalTok{hier}\StringTok{", "}\NormalTok{hin}\StringTok{", "}\NormalTok{hinter}\StringTok{", "}\NormalTok{ich}\StringTok{", "}\NormalTok{ihm}\StringTok{", "}\NormalTok{ihn}\StringTok{", "}\NormalTok{ihnen}\StringTok{", "}\NormalTok{ihr}\StringTok{", "}\NormalTok{ihre}\StringTok{", "}\NormalTok{ihrem}\StringTok{", "}\NormalTok{ihren}\StringTok{", "}\NormalTok{ihrer}\StringTok{", "}\NormalTok{im}\StringTok{", "}\NormalTok{in}\StringTok{", "}\NormalTok{ins}\StringTok{", "}\NormalTok{is}\StringTok{", "}\NormalTok{ist}\StringTok{", "}\NormalTok{ja}\StringTok{", "}\NormalTok{je}\StringTok{", "}\NormalTok{jede}\StringTok{", "}\NormalTok{jedem}\StringTok{", "}\NormalTok{jeden}\StringTok{", "}\NormalTok{jeder}\StringTok{", "}\NormalTok{jedes}\StringTok{", "}\NormalTok{jetzt}\StringTok{", "}\NormalTok{kann}\StringTok{", "}\NormalTok{kannst}\StringTok{", "}\NormalTok{kein}\StringTok{", "}\NormalTok{keine}\StringTok{", "}\NormalTok{keinem}\StringTok{", "}\NormalTok{keinen}\StringTok{", "}\NormalTok{keiner}\StringTok{", "}\NormalTok{kÃ¶nnen}\StringTok{", "}\NormalTok{kÃ¶nnt}\StringTok{", "}\NormalTok{konnte}\StringTok{", "}\NormalTok{kÃ¶nnte}\StringTok{", "}\NormalTok{kÃ¶nnten}\StringTok{", "}\NormalTok{mach}\StringTok{", "}\NormalTok{mache}\StringTok{", "}\NormalTok{machen}\StringTok{", "}\NormalTok{machst}\StringTok{", "}\NormalTok{macht}\StringTok{", "}\NormalTok{mal}\StringTok{", "}\NormalTok{man}\StringTok{", "}\NormalTok{manche}\StringTok{", "}\NormalTok{mein}\StringTok{", "}\NormalTok{meine}\StringTok{", "}\NormalTok{meinem}\StringTok{", "}\NormalTok{meinen}\StringTok{", "}\NormalTok{meiner}\StringTok{", "}\NormalTok{meines}\StringTok{", "}\NormalTok{mich}\StringTok{", "}\NormalTok{mir}\StringTok{", "}\NormalTok{mit}\StringTok{", "}\NormalTok{muss}\StringTok{", "}\NormalTok{mÃ¼ssen", }\StringTok{"musst"}\NormalTok{, }\StringTok{"musste"}\NormalTok{, }\StringTok{"mÃ¼sste", "}\NormalTok{mussten}\StringTok{", "}\NormalTok{na}\StringTok{", "}\NormalTok{nach}\StringTok{", "}\NormalTok{nachdem}\StringTok{", "}\NormalTok{naja}\StringTok{", "}\NormalTok{ne}\StringTok{", "}\NormalTok{nein}\StringTok{", "}\NormalTok{nem}\StringTok{", "}\NormalTok{nen}\StringTok{", "}\NormalTok{ner}\StringTok{", "}\NormalTok{nicht}\StringTok{", "}\NormalTok{nichts}\StringTok{", "}\NormalTok{nix}\StringTok{", "}\NormalTok{noch}\StringTok{", "}\NormalTok{nun}\StringTok{", "}\NormalTok{nur}\StringTok{", "}\NormalTok{ob}\StringTok{", "}\NormalTok{oder}\StringTok{", "}\NormalTok{ohne}\StringTok{", "}\NormalTok{ok}\StringTok{", "}\NormalTok{okay}\StringTok{", "}\NormalTok{raus}\StringTok{", "}\NormalTok{rein}\StringTok{", "}\NormalTok{rum}\StringTok{", "}\NormalTok{schon}\StringTok{", "}\NormalTok{sehr}\StringTok{", "}\NormalTok{sei}\StringTok{", "}\NormalTok{seid}\StringTok{", "}\NormalTok{sein}\StringTok{", "}\NormalTok{seine}\StringTok{", "}\NormalTok{seinem}\StringTok{", "}\NormalTok{seinen}\StringTok{", "}\NormalTok{seiner}\StringTok{", "}\NormalTok{selber}\StringTok{", "}\NormalTok{selbst}\StringTok{", "}\NormalTok{sich}\StringTok{", "}\NormalTok{sie}\StringTok{", "}\NormalTok{sind}\StringTok{", "}\NormalTok{so}\StringTok{", "}\NormalTok{solche}\StringTok{", "}\NormalTok{solchen}\StringTok{", "}\NormalTok{soll}\StringTok{", "}\NormalTok{sollen}\StringTok{", "}\NormalTok{sollte}\StringTok{", "}\NormalTok{sollten}\StringTok{", "}\NormalTok{solltest}\StringTok{", "}\NormalTok{somit}\StringTok{", "}\NormalTok{sondern}\StringTok{", "}\NormalTok{sonst}\StringTok{", "}\NormalTok{sowas}\StringTok{", "}\NormalTok{soweit}\StringTok{", "}\NormalTok{tun}\StringTok{", "}\NormalTok{tut}\StringTok{", "}\NormalTok{Ã¼ber}\StringTok{", "}\NormalTok{um}\StringTok{", "}\NormalTok{und}\StringTok{", "}\NormalTok{uns}\StringTok{", "}\NormalTok{unser}\StringTok{", "}\NormalTok{unsere}\StringTok{", "}\NormalTok{unserem}\StringTok{", "}\NormalTok{unseren}\StringTok{", "}\NormalTok{unserer}\StringTok{", "}\NormalTok{unter}\StringTok{", "}\NormalTok{usw}\StringTok{", "}\NormalTok{viel}\StringTok{", "}\NormalTok{viele}\StringTok{", "}\NormalTok{vielen}\StringTok{", "}\NormalTok{vieles}\StringTok{", "}\NormalTok{vom}\StringTok{", "}\NormalTok{von}\StringTok{", "}\NormalTok{vor}\StringTok{", "}\NormalTok{war}\StringTok{", "}\NormalTok{wÃ¤re}\StringTok{", "}\NormalTok{waren}\StringTok{", "}\NormalTok{wÃ¤ren}\StringTok{", "}\NormalTok{wars}\StringTok{", "}\NormalTok{was}\StringTok{", "}\NormalTok{weder}\StringTok{", "}\NormalTok{weg}\StringTok{", "}\NormalTok{wegen}\StringTok{", "}\NormalTok{weil}\StringTok{", "}\NormalTok{weiter}\StringTok{", "}\NormalTok{weitere}\StringTok{", "}\NormalTok{welche}\StringTok{", "}\NormalTok{welchen}\StringTok{", "}\NormalTok{welcher}\StringTok{", "}\NormalTok{welches}\StringTok{", "}\NormalTok{wenn}\StringTok{", "}\NormalTok{wenns}\StringTok{", "}\NormalTok{wer}\StringTok{", "}\NormalTok{werd}\StringTok{", "}\NormalTok{werde}\StringTok{", "}\NormalTok{werden}\StringTok{", "}\NormalTok{wie}\StringTok{", "}\NormalTok{wieder}\StringTok{", "}\NormalTok{wieso}\StringTok{", "}\NormalTok{will}\StringTok{", "}\NormalTok{willst}\StringTok{", "}\NormalTok{wir}\StringTok{", "}\NormalTok{wird}\StringTok{", "}\NormalTok{wirst}\StringTok{", "}\NormalTok{wo}\StringTok{", "}\NormalTok{wobei}\StringTok{", "}\NormalTok{wollen}\StringTok{", "}\NormalTok{wollte}\StringTok{", "}\NormalTok{wollten}\StringTok{", "}\NormalTok{worden}\StringTok{", "}\NormalTok{wurde}\StringTok{", "}\NormalTok{wÃ¼rde", }\StringTok{"wurden"}\NormalTok{, }\StringTok{"wÃ¼rden", "}\NormalTok{z.b}\StringTok{", "}\NormalTok{zb}\StringTok{", "}\NormalTok{zu}\StringTok{", "}\NormalTok{zum}\StringTok{", "}\NormalTok{zur}\StringTok{", "}\NormalTok{zwar}\StringTok{", "}\NormalTok{zwischen}\StringTok{")}

\StringTok{# Kombinationen von WÃ¶rtern}
\StringTok{relevant_ngrams = dictionary(list(}
\StringTok{  "}\NormalTok{trotz_impfung}\StringTok{" = "}\NormalTok{trotz impfung}\StringTok{",}
\StringTok{  "}\NormalTok{grippe_impfen}\StringTok{" = "}\NormalTok{grippe impfen}\StringTok{",}
\StringTok{  "}\NormalTok{mmr_impfung}\StringTok{" = "}\NormalTok{mmr impfung}\StringTok{",}
\StringTok{  "}\NormalTok{hepatitis_b}\StringTok{" = "}\NormalTok{hepatitis b}\StringTok{",}
\StringTok{  "}\NormalTok{gut_vertragen}\StringTok{" = "}\NormalTok{gut vertragen}\StringTok{",}
\StringTok{  "}\NormalTok{6fach_impfung}\StringTok{" = "}\NormalTok{6fach impfung}\StringTok{",}
\StringTok{  "}\DecValTok{6}\NormalTok{_fach}\StringTok{" = "}\DecValTok{6}\NormalTok{ fach}\StringTok{",}
\StringTok{  "}\DecValTok{6}\NormalTok{_fach_impfung}\StringTok{" = "}\DecValTok{6}\NormalTok{ fach impfung}\StringTok{",}
\StringTok{  "}\NormalTok{meningokokken_b}\StringTok{" = "}\NormalTok{meningokokken b}\StringTok{",}
\StringTok{  "}\NormalTok{gute_besserung}\StringTok{" = "}\NormalTok{gute besserung}\StringTok{",}
\StringTok{  "}\DecValTok{6}\OperatorTok{-}\NormalTok{fach_impfung}\StringTok{" = "}\DecValTok{6}\OperatorTok{-}\NormalTok{fach impfung}\StringTok{",}
\StringTok{  "}\NormalTok{erhÃ¶hte_temperatur}\StringTok{" = "}\NormalTok{erhÃ¶hte temperatur}\StringTok{",}
\StringTok{  "}\NormalTok{kein_fieber}\StringTok{" = "}\NormalTok{kein fieber}\StringTok{",}
\StringTok{  "}\NormalTok{kein_problem}\StringTok{" = "}\NormalTok{kein problem}\StringTok{",}
\StringTok{  "}\NormalTok{keine_ahnung}\StringTok{" = "}\NormalTok{keine ahnung}\StringTok{",}
\StringTok{  "}\NormalTok{keine_impfung}\StringTok{" = "}\NormalTok{keine impfung}\StringTok{",}
\StringTok{  "}\NormalTok{nicht_geimpft}\StringTok{" = "}\NormalTok{nicht geimpft}\StringTok{",}
\StringTok{  "}\NormalTok{nicht_impfen}\StringTok{" = "}\NormalTok{nicht impfen}\StringTok{",}
\StringTok{  "}\NormalTok{nicht_zu_impfen}\StringTok{" = "}\NormalTok{nicht zu impfen}\StringTok{",}
\StringTok{  "}\NormalTok{selbst_entscheiden}\StringTok{" = "}\NormalTok{selbst entscheiden}\StringTok{"}
\StringTok{))}

\StringTok{# Erstellen einer Dokument-Feature-Matrix aus dem Korpus}
\StringTok{impf_dfm = crps %>% }
\StringTok{  dfm(stem = FALSE, tolower = TRUE, remove_punct = TRUE,}
\StringTok{      remove = custom_stopwords,}
\StringTok{      remove_url = TRUE, verbose = TRUE,}
\StringTok{      thesaurus = relevant_ngrams)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Creating a dfm from a corpus input...
\end{verbatim}

\begin{verbatim}
##    ... lowercasing
\end{verbatim}

\begin{verbatim}
##    ... found 12,369 documents, 41,651 features
\end{verbatim}

\begin{verbatim}
##    ... applying a dictionary consisting of 20 keys
##    ... removed 286 features
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{impf_dfm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Document-feature matrix of: 12,369 documents, 41,385 features (99.9% sparse) and 6 docvars.
##        features
## docs    TROTZ_IMPFUNG GRIPPE_IMPFEN MMR_IMPFUNG HEPATITIS_B GUT_VERTRAGEN
##   text1             0             0           0           0             0
##   text2             0             0           0           0             0
##   text3             0             0           0           0             0
##   text4             1             0           0           0             0
##   text5             0             0           0           0             0
##   text6             0             0           0           0             0
##        features
## docs    6FACH_IMPFUNG 6_FACH 6_FACH_IMPFUNG MENINGOKOKKEN_B GUTE_BESSERUNG
##   text1             0      0              0               0              0
##   text2             0      0              0               0              0
##   text3             0      0              0               0              0
##   text4             1      0              0               0              0
##   text5             0      0              0               0              0
##   text6             0      0              0               0              0
## [ reached max_ndoc ... 12,363 more documents, reached max_nfeat ... 41,375 more features ]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Pruning}
\NormalTok{impf_dfm =}\StringTok{ }\NormalTok{impf_dfm }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{dfm_trim}\NormalTok{(}\DataTypeTok{max_docfreq =} \FloatTok{0.99}\NormalTok{, }\DataTypeTok{min_docfreq =} \FloatTok{0.005}\NormalTok{, }\DataTypeTok{docfreq_type =} \StringTok{"prop"}\NormalTok{)}
\NormalTok{impf_dfm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Document-feature matrix of: 12,369 documents, 1,150 features (98.2% sparse) and 6 docvars.
##        features
## docs    TROTZ_IMPFUNG GRIPPE_IMPFEN MMR_IMPFUNG GUT_VERTRAGEN 6_FACH
##   text1             0             0           0             0      0
##   text2             0             0           0             0      0
##   text3             0             0           0             0      0
##   text4             1             0           0             0      0
##   text5             0             0           0             0      0
##   text6             0             0           0             0      0
##        features
## docs    MENINGOKOKKEN_B GUTE_BESSERUNG ERHÃ–HTE_TEMPERATUR KEIN_FIEBER
##   text1               0              0                  0           0
##   text2               0              0                  0           0
##   text3               0              0                  0           0
##   text4               0              0                  0           0
##   text5               0              0                  0           0
##   text6               0              0                  0           0
##        features
## docs    KEIN_PROBLEM
##   text1            0
##   text2            0
##   text3            0
##   text4            0
##   text5            0
##   text6            0
## [ reached max_ndoc ... 12,363 more documents, reached max_nfeat ... 1,140 more features ]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Ãœberblick: Die hÃ¤ufigsten Features im Korpus}
\NormalTok{impf_dfm }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{colSums}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{enframe}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(value)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r}
\hline
name & value\\
\hline
impfung & 4519\\
\hline
impfen & 4356\\
\hline
kind & 3690\\
\hline
lassen & 3257\\
\hline
immer & 2705\\
\hline
mehr & 2594\\
\hline
kinder & 2503\\
\hline
gibt & 2184\\
\hline
geimpft & 2176\\
\hline
impfungen & 2174\\
\hline
gut & 2115\\
\hline
hallo & 1898\\
\hline
einfach & 1767\\
\hline
lg & 1720\\
\hline
2 & 1704\\
\hline
bekommen & 1585\\
\hline
ganz & 1584\\
\hline
erst & 1558\\
\hline
geht & 1492\\
\hline
arzt & 1411\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# als (beliebte, wenn auch nur mittel informative) Wordcloud}
\NormalTok{impf_dfm }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{textplot_wordcloud}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜Š' in 'mbcsToSbcs': Punkt ersetzt <f0>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜Š' in 'mbcsToSbcs': Punkt ersetzt <9f>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜Š' in 'mbcsToSbcs': Punkt ersetzt <98>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜Š' in 'mbcsToSbcs': Punkt ersetzt <8a>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜Š' in 'mbcsToSbcs': Punkt ersetzt <f0>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜Š' in 'mbcsToSbcs': Punkt ersetzt <9f>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜Š' in 'mbcsToSbcs': Punkt ersetzt <98>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜Š' in 'mbcsToSbcs': Punkt ersetzt <8a>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Fontmetrik ist fÃ¼r das Unicode-Zeichen U+1f60a unbekannt
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜‚' in 'mbcsToSbcs': Punkt ersetzt <f0>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜‚' in 'mbcsToSbcs': Punkt ersetzt <9f>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜‚' in 'mbcsToSbcs': Punkt ersetzt <98>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜‚' in 'mbcsToSbcs': Punkt ersetzt <82>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜‚' in 'mbcsToSbcs': Punkt ersetzt <f0>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜‚' in 'mbcsToSbcs': Punkt ersetzt <9f>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜‚' in 'mbcsToSbcs': Punkt ersetzt <98>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜‚' in 'mbcsToSbcs': Punkt ersetzt <82>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Fontmetrik ist fÃ¼r das Unicode-Zeichen U+1f602 unbekannt
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜‰' in 'mbcsToSbcs': Punkt ersetzt <f0>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜‰' in 'mbcsToSbcs': Punkt ersetzt <9f>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜‰' in 'mbcsToSbcs': Punkt ersetzt <98>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜‰' in 'mbcsToSbcs': Punkt ersetzt <89>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜‰' in 'mbcsToSbcs': Punkt ersetzt <f0>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜‰' in 'mbcsToSbcs': Punkt ersetzt <9f>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜‰' in 'mbcsToSbcs': Punkt ersetzt <98>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜‰' in 'mbcsToSbcs': Punkt ersetzt <89>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Fontmetrik ist fÃ¼r das Unicode-Zeichen U+1f609 unbekannt
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ™ˆ' in 'mbcsToSbcs': Punkt ersetzt <f0>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ™ˆ' in 'mbcsToSbcs': Punkt ersetzt <9f>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ™ˆ' in 'mbcsToSbcs': Punkt ersetzt <99>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ™ˆ' in 'mbcsToSbcs': Punkt ersetzt <88>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ™ˆ' in 'mbcsToSbcs': Punkt ersetzt <f0>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ™ˆ' in 'mbcsToSbcs': Punkt ersetzt <9f>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ™ˆ' in 'mbcsToSbcs': Punkt ersetzt <99>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ™ˆ' in 'mbcsToSbcs': Punkt ersetzt <88>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Fontmetrik ist fÃ¼r das Unicode-Zeichen U+1f648 unbekannt
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜…' in 'mbcsToSbcs': Punkt ersetzt <f0>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜…' in 'mbcsToSbcs': Punkt ersetzt <9f>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜…' in 'mbcsToSbcs': Punkt ersetzt <98>
\end{verbatim}

\begin{verbatim}
## Warning in graphics::strwidth(word[i], cex = size[i]): Konvertierungsfehler fÃ¼r
## 'ğŸ˜…' in 'mbcsToSbcs': Punkt ersetzt <85>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜…' in 'mbcsToSbcs': Punkt ersetzt <f0>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜…' in 'mbcsToSbcs': Punkt ersetzt <9f>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜…' in 'mbcsToSbcs': Punkt ersetzt <98>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Konvertierungsfehler fÃ¼r 'ğŸ˜…' in 'mbcsToSbcs': Punkt ersetzt <85>
\end{verbatim}

\begin{verbatim}
## Warning in text.default(x1, y1, word[i], cex = (1 + adjust) * size[i], offset =
## 0, : Fontmetrik ist fÃ¼r das Unicode-Zeichen U+1f605 unbekannt
\end{verbatim}

\includegraphics{workshop_topicmodels_files/figure-latex/prepare-data-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Umwandeln der dfm in das Format fÃ¼r stm}
\NormalTok{impf_stm =}\StringTok{ }\NormalTok{impf_dfm }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{quanteda}\OperatorTok{::}\KeywordTok{convert}\NormalTok{(}\DataTypeTok{to =} \StringTok{"stm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in dfm2stm(x, docvars, omit_empty = TRUE): Dropped empty document(s):
## text12225, text12271
\end{verbatim}

\hypertarget{modellspezifikation-modellvergleich-und-modellauswahl}{%
\chapter{Modellspezifikation, Modellvergleich und Modellauswahl}\label{modellspezifikation-modellvergleich-und-modellauswahl}}

\hypertarget{modellspezifikation}{%
\section{Modellspezifikation}\label{modellspezifikation}}

\begin{itemize}
\item
  Die Funktion \texttt{stm()} aus dem gleichnamigen Paket bietet zahlreiche MÃ¶glichkeiten, Details der Modellspezifikation und -schÃ¤tzung anzupassen. Wir beschrÃ¤nken uns im Folgenden auf drei wesentliche Einstellungen:

  \begin{itemize}
  \tightlist
  \item
    \texttt{K}: Die Zahl der Topics.
  \item
    \texttt{prevalence}: Eine Formel zur Vorhersage der Topic-PrÃ¤valenzen
  \item
    \texttt{init.type}: Wie soll der Startpunkt fÃ¼r die ModellschÃ¤tzung gewÃ¤hlt werden?
  \end{itemize}
\item
  Zu weiteren Details siehe fÃ¼r einen Ãœberblick \texttt{?stm} und \citet{robertsStmPackageStructural2019} fÃ¼r eine ausfÃ¼hrliche ErlÃ¤uterung.
\item
  Eine Modell-Spezifikation kÃ¶nnte z.B. so aussehen:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelfit =}\StringTok{ }\KeywordTok{stm}\NormalTok{(}\DataTypeTok{documents =}\NormalTok{ impf_stm}\OperatorTok{$}\NormalTok{documents,}
               \DataTypeTok{vocab =}\NormalTok{ impf_stm}\OperatorTok{$}\NormalTok{vocab,}
               \DataTypeTok{data =}\NormalTok{ impf_stm}\OperatorTok{$}\NormalTok{meta, }
               \DataTypeTok{K =} \DecValTok{10}\NormalTok{, }
               \DataTypeTok{prevalence =} \OperatorTok{~}\KeywordTok{s}\NormalTok{(date_num), }
               \DataTypeTok{init.type =} \StringTok{"Spectral"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Mit den ersten drei Inputs Ã¼bergeben wir die Daten aus der im letzten Abschnitt erstellten Dokument-Feature-Matrix.
\item
  Mit \texttt{K} geben wir an, wie viele Themen es geben soll. Mit dieser Syntax wÃ¼rde ein Modell mit \(k = 10\) Themen geschÃ¤tzt. Wie wir bei der Wahl eines geeigneten \(k\) vorgehen kÃ¶nnen, ist Thema des folgenden Unterabschnitts.
\item
  Mit der Formel zu \texttt{prevalence} geben wir an, welche Dokument-Variablen mit dem Auftreten der Topics zusammenhÃ¤ngen.

  \begin{itemize}
  \tightlist
  \item
    In der Formel wird die abhÃ¤ngige Variable vor der Tilde (\textasciitilde) frei gelassen. Es wird immer der Zusammenhang mit dem Auftreten von allen \(k\) Topics geschÃ¤tzt. In diesem Beispiel schÃ¤tzen wir, wie sich das Auftreten der Topics Ã¼ber den Untersuchungszeitraum hinweg verÃ¤ndert. Details zum SchÃ¤tzen von ZusammenhÃ¤ngen mit Kovariaten folgen spÃ¤ter in diesem Workshop.
  \item
    Mit \texttt{init.type} wird angegeben, wie \texttt{stm()} die Ausgangswerte fÃ¼r die ModellschÃ¤tzung bestimmen soll. Die Default-Einstellung ist ``Spectral''. Ich empfehle diese Einstellung aus folgenden GrÃ¼nden:

    \begin{itemize}
    \tightlist
    \item
      Sie ist deterministisch, d.h., sie fÃ¼hrt gegeben derselben Daten und desselben Modells immer zu derselben LÃ¶sung. So wird die Reproduzierbarkeit sichergestellt.
    \item
      Sie ist effizient, d.h., dass von diesem Startpunkt aus relativ schnell die finale LÃ¶sung gefunden wird.
    \item
      Wenn eine andere Einstellung fÃ¼r die Ausgangswerte gewÃ¤hlt wird, mÃ¼ssen mehrere SchÃ¤tzungen fÃ¼r eine Spezifikation durchgefÃ¼hrt werden. Nur so kann geprÃ¼ft werden, ob die Ausgangswerte das Ergebnis beeinflussen.
    \end{itemize}
  \end{itemize}
\item
  Allgemein muss beachtet werden, dass die SchÃ¤tzung eines Structural Topic Model mit \texttt{stm()} trotz der Effizienz der Implementierung sehr rechenintensiv ist. Die SchÃ¤tzung des oben beschriebenen Modells dauert auf meinem recht leistungsfÃ¤higen Notebook bereits ca. eine Minute. Es empfiehlt sich daher, die Modelle immer in neue Objekte zu speichern und diese ggf. direkt auf der Festplatte zu sichern. Um die Berechnungszeiten in diesem Workshop kurz zu halten, stelle ich die Ergebnisse der ModellschÃ¤tzungen Ã¼ber das LMS zur VerfÃ¼gung. Wenn ihr diese herunterladet und in den Ordner ``data'' kopiert, muss das Modell nicht neu geschÃ¤tzt werden.
\end{itemize}

\hypertarget{modellvergleich}{%
\section{Modellvergleich}\label{modellvergleich}}

\hypertarget{allgemeines-vorgehen}{%
\subsection*{Allgemeines Vorgehen}\label{allgemeines-vorgehen}}
\addcontentsline{toc}{subsection}{Allgemeines Vorgehen}

\begin{itemize}
\tightlist
\item
  Eine zentrale Frage ist die Wahl eines geeigneten \(k\), also der Zahl von Topics, die in den Dokumenten identifiziert werden sollen. Wichtig ist zuerst die Feststellung, dass es in der angewandten Analyse kein \emph{per se} richtiges oder falsches \(k\) gibt.
\item
  Wie viele Topics nÃ¼tzlich sind, hÃ¤ngt von Umfang von Zusammensetzung des Materials und vom substantiellen Forschungsinteresse ab.
\item
  Um ein geeignetes \(k\) zu finden, gehen wir in der Regel modellvergleichend vor. Wir schÃ¤tzen Modelle mit unterschiedlich vielen Topics und prÃ¼fen dann, welche Modelle besser zu den Daten und zum Forschungsinteresse passen.
\item
  Hinweise fÃ¼r einen allgemeinen Ausgangspunkt, in welchem Bereich nÃ¼tzliche \(k\) zu finden sein kÃ¶nnten, liefert die Paket-Hilfe:
\end{itemize}

\begin{quote}
The most important user input in parametric topic models is the number of topics. There is no right answer to the appropriate number of topics. More topics will give more fine-grained representations of the data at the potential cost of being less precisely estimated. {[}\ldots{]} For short corpora focused on very specific subject matter (such as survey experiments) 3-10 topics is a useful starting range. For small corpora (a few hundred to a few thousand) 5-50 topics is a good place to start. Beyond these rough guidelines it is application specific. Previous applications in political science with medium sized corpora (10k to 100k documents) have found 60-100 topics to work well. For larger corpora 100 topics is a useful default size. Of course, your mileage may vary. --- ?stm
\end{quote}

\begin{itemize}
\tightlist
\item
  Hier werden zwei wichtige Kriterien, die unser Nachdenken Ã¼ber die Spannweite von zu BerÃ¼cksichtigten \(k\) leiten kÃ¶nnen, deutlich:

  \begin{itemize}
  \tightlist
  \item
    QuantitÃ¤t des Materials: Je mehr Dokumente, desto mehr Topics.
  \item
    Varianz im Inhalt: Je mehr inhaltliche Varianz, desto mehr Topics (an einem Beispiel: fÃ¼r 10k NachrichtenbeitrÃ¤ge aus dem Wirtschaftsteil brauchen wir weniger Topics als fÃ¼r 10k NachrichtenbeitrÃ¤ge, die aus allen Ressorts kommen).
  \end{itemize}
\item
  Im vorliegenden Fall haben wir einen kleinen bis mittleren Korpus (ca. 13k Dokumente, die grÃ¶ÃŸtenteils recht kurz sind). Wir kÃ¶nnen von einer mittleren inhaltlichen Varianz ausgehen. Einerseits haben wir Posts bewusst danach ausgewÃ¤hlt, dass sie sich mit dem Thema Impfen beschÃ¤ftigen, was die Varianz einschrÃ¤nkt. Andererseits wissen wir, dass in Online-Foren die verschiedensten Perspektiven auf dieses Thema vorkommen kÃ¶nnen, was fÃ¼r Varianz sorgt.
\item
  Wir gehen im Folgenden in mehreren Schritten vor:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi})}
  \tightlist
  \item
    Um eine allgemeine Orientierung zu erhalten, in welcher Range Modelle zu finden sind, die gut zu den Daten passen, schÃ¤tzen wir 10 Modelle von \(k = 10\) bis \(k = 100\) mit einem Abstand von jeweils 10 Topics. Diese Modelle vergleichen wir anhand von einigen statistischen MaÃŸen, um die Zahl der Kandidatenmodelle einzuschrÃ¤nken.
  \item
    Wir interpretieren die besten Modelle substantiell und entscheiden, welche Topic-Anzahl fÃ¼r das Forschungsinteresse hilfreicher scheint.
  \item
    Wir schÃ¤tzen weitere Modelle basierend auf den Ergebnissen aus 2) mit kleineren AbstÃ¤nden zwischen den \(k\). Wir prÃ¼fen, wie sich die Topics verÃ¤ndern. Zudem achten wir darauf, ob bei Modellen mit grÃ¶ÃŸeren \(k\) interessante Topics hinzukommen oder ob sich mehr Ambivalenzen zeigen.
  \item
    Wir entscheiden uns fÃ¼r ein Modell. Dieses beschreiben wir dann ausfÃ¼hrlich.
  \end{enumerate}
\item
  Da das SchÃ¤tzen der Modelle recht lange dauert, parallelisieren wir die Berechnung. Dazu nutze ich das Paket \texttt{furrr}. Es sei an dieser Stelle darauf hingewiesen, dass das Paket vor allem unter Windows mit RStudio fÃ¼r Probleme sorgen kann. Es ist daher empfehlenswert, das Skript zum SchÃ¤tzen und Speichern der Modelle in der Konsole oder im Terminal auszufÃ¼hren. Noch schneller geht es fÃ¼r diesen Workshop, die bereits geschÃ¤tzten Modelle aus dem LMS zu laden.
\end{itemize}

\hypertarget{quantitativer-vergleich-der-ersten-modelle}{%
\subsection*{Quantitativer Vergleich der ersten Modelle}\label{quantitativer-vergleich-der-ersten-modelle}}
\addcontentsline{toc}{subsection}{Quantitativer Vergleich der ersten Modelle}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 1) Modelle mit K = 10, ..., K = 100}
\CommentTok{# Modelle schÃ¤tzen bzw. laden}
\ControlFlowTok{if}\NormalTok{ (}\KeywordTok{file.exists}\NormalTok{(}\StringTok{"R/data/models10_100.rds"}\NormalTok{)) \{}
\CommentTok{# Schneller: Modelle aus LMS laden}
\NormalTok{  many_models =}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"R/data/models10_100.rds"}\NormalTok{)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
\CommentTok{# Vorsicht: SchÃ¤tzen dauert auf meinem MacBook Pro 2020 i9 32 GB RAM 10 Minuten}
\NormalTok{  Ks =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DataTypeTok{by =} \DecValTok{10}\NormalTok{)}
  \KeywordTok{tic}\NormalTok{()}
  \KeywordTok{plan}\NormalTok{(}\KeywordTok{multiprocess}\NormalTok{(}\DataTypeTok{workers =} \DecValTok{10}\NormalTok{))}
\NormalTok{  many_models =}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{K =}\NormalTok{ Ks) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{topic_model =} \KeywordTok{future_map}\NormalTok{(K, }\OperatorTok{~}\KeywordTok{stm}\NormalTok{(}\DataTypeTok{documents =}\NormalTok{ impf_stm}\OperatorTok{$}\NormalTok{documents, }
                                            \DataTypeTok{vocab =}\NormalTok{ impf_stm}\OperatorTok{$}\NormalTok{vocab,}
                                            \DataTypeTok{data =}\NormalTok{ impf_stm}\OperatorTok{$}\NormalTok{meta,}
                                            \DataTypeTok{init.type =} \StringTok{"Spectral"}\NormalTok{,}
                                            \DataTypeTok{K =}\NormalTok{ ., }\DataTypeTok{verbose =} \OtherTok{FALSE}\NormalTok{),}
                                    \DataTypeTok{.progress =} \OtherTok{TRUE}\NormalTok{))}
  \KeywordTok{plan}\NormalTok{(sequential)}
  \KeywordTok{toc}\NormalTok{()}
  \KeywordTok{saveRDS}\NormalTok{(many_models, }\StringTok{"R/data/models10_100.rds"}\NormalTok{)  }
\NormalTok{\}}

\CommentTok{# Quantitative Indikatoren der ModellqualitÃ¤t berechnen}
\CommentTok{# Inspired by https://juliasilge.com/blog/evaluating-stm/}
\ControlFlowTok{if}\NormalTok{ (}\KeywordTok{file.exists}\NormalTok{(}\StringTok{"R/data/eval10_100.rds"}\NormalTok{)) \{}
  \CommentTok{# Schneller: Modellevaluation aus LMS laden}
\NormalTok{  model_eval =}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"R/data/eval10_100.rds"}\NormalTok{)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \CommentTok{# Evaluation dauert auf meinem MacBook Pro 2020 i9 32 GB RAM 24 Sekunden}
  \KeywordTok{tic}\NormalTok{()}
\NormalTok{  heldout =}\StringTok{ }\KeywordTok{make.heldout}\NormalTok{(}\DataTypeTok{documents =}\NormalTok{ impf_stm}\OperatorTok{$}\NormalTok{documents, }\DataTypeTok{vocab =}\NormalTok{ impf_stm}\OperatorTok{$}\NormalTok{vocab)}
  \KeywordTok{plan}\NormalTok{(}\KeywordTok{multiprocess}\NormalTok{(}\DataTypeTok{workers =} \DecValTok{10}\NormalTok{))}
\NormalTok{  model_eval =}\StringTok{ }\NormalTok{many_models }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{exclusivity =} \KeywordTok{future_map}\NormalTok{(topic_model, exclusivity),}
           \DataTypeTok{semantic_coherence =} \KeywordTok{future_map}\NormalTok{(topic_model, semanticCoherence, impf_stm}\OperatorTok{$}\NormalTok{documents),}
           \DataTypeTok{eval_heldout =} \KeywordTok{future_map}\NormalTok{(topic_model, eval.heldout, heldout}\OperatorTok{$}\NormalTok{missing),}
           \DataTypeTok{residual =} \KeywordTok{future_map}\NormalTok{(topic_model, checkResiduals, impf_stm}\OperatorTok{$}\NormalTok{documents),}
           \DataTypeTok{residuals =} \KeywordTok{future_map_dbl}\NormalTok{(residual, }\StringTok{"dispersion"}\NormalTok{),}
           \DataTypeTok{held_out_likelihood =} \KeywordTok{future_map_dbl}\NormalTok{(eval_heldout, }\StringTok{"expected.heldout"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{topic_model)}
  \KeywordTok{plan}\NormalTok{(sequential)}
  \KeywordTok{toc}\NormalTok{()}
  \KeywordTok{saveRDS}\NormalTok{(model_eval, }\StringTok{"R/data/eval10_100.rds"}\NormalTok{)  }
\NormalTok{\}}

\CommentTok{# ExklusivitÃ¤t und Semantische KohÃ¤renz (Mean, Median)}
\NormalTok{model_eval }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(K, }\DataTypeTok{Exclusivity =}\NormalTok{ exclusivity, }\DataTypeTok{Coherence =}\NormalTok{ semantic_coherence) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate_at}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DataTypeTok{.funs =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{Mean =} \OperatorTok{~}\KeywordTok{map_dbl}\NormalTok{(.x, mean), }\DataTypeTok{Median =} \OperatorTok{~}\KeywordTok{map_dbl}\NormalTok{(.x, median))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select_if}\NormalTok{(}\KeywordTok{negate}\NormalTok{(is.list)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{(metric, value, }\OperatorTok{-}\NormalTok{K) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(metric, }\KeywordTok{c}\NormalTok{(}\StringTok{"measure"}\NormalTok{, }\StringTok{"metric"}\NormalTok{), }\StringTok{"_"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{spread}\NormalTok{(measure, value) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Coherence, Exclusivity, }\DataTypeTok{label =}\NormalTok{ K)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_text}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{facet_wrap}\NormalTok{(}\StringTok{"metric"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{workshop_topicmodels_files/figure-latex/model-compare-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ExklusivitÃ¤t und Semantische KohÃ¤renz von k = 40, 50}
\NormalTok{model_eval }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(K }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(K, }\DataTypeTok{Exclusivity =}\NormalTok{ exclusivity, }\DataTypeTok{Coherence =}\NormalTok{ semantic_coherence) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest}\NormalTok{(}\KeywordTok{c}\NormalTok{(Exclusivity, Coherence)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Coherence, Exclusivity, }\DataTypeTok{color =} \KeywordTok{factor}\NormalTok{(K))) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{workshop_topicmodels_files/figure-latex/model-compare-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Held-out-likelihood und multinomiale Residuen}
\NormalTok{model_eval }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(K, }\StringTok{`}\DataTypeTok{Held-out likelihood (higher is better)}\StringTok{`}\NormalTok{ =}\StringTok{ }\NormalTok{held_out_likelihood, }\StringTok{`}\DataTypeTok{Multinomial dispersion of residuals (lower is better)}\StringTok{`}\NormalTok{ =}\StringTok{ }\NormalTok{residuals) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{(measure, value, }\OperatorTok{-}\NormalTok{K) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(K, value, }\DataTypeTok{label =}\NormalTok{ K)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_text}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{facet_wrap}\NormalTok{(}\StringTok{"measure"}\NormalTok{, }\DataTypeTok{scales =} \StringTok{"free_y"}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Number of topics (K)"}\NormalTok{, }\DataTypeTok{y =} \OtherTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{workshop_topicmodels_files/figure-latex/model-compare-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Speichern der Modelle mit 30 und 40 Topics fÃ¼r qualitative Analyse}
\CommentTok{# Benennung und Datenstruktur fÃ¼r stminsights}
\NormalTok{out =}\StringTok{ }\NormalTok{impf_stm}
\NormalTok{m30 =}\StringTok{ }\NormalTok{many_models}\OperatorTok{$}\NormalTok{topic_model[[}\DecValTok{3}\NormalTok{]]}
\NormalTok{m60 =}\StringTok{ }\NormalTok{many_models}\OperatorTok{$}\NormalTok{topic_model[[}\DecValTok{6}\NormalTok{]]}
\CommentTok{# save(out, m30, m60, file = "R/data/models30_60.rdata")}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Wir betrachten zuerst die \emph{semantische KohÃ¤renz} und die \emph{ExklusivitÃ¤t} der Topics in den Modellen. Beide Metriken sind Eigenschaften der einzelnen Themen. In der ersten Grafik sind daher der Mittelwerte und die Mediane aller Themen in einem Modell dargestellt. Die absoluten Werte beider Metriken haben keine substantielle Bedeutung. Von Interesse ist der Vergleich der Modelle. Je hÃ¶her die ExklusivitÃ¤t, desto geringer ist die Wahrscheinlichkeit, dass die typischsten Terme eines Topics in anderen Topics vorkommen \citep{robertsStructuralTopicModels2014}. Je hÃ¶her die semantische KohÃ¤renz, desto wahrscheinlicher kommen die typischsten WÃ¶rter eines Topics gemeinsam in einem Dokument mit diesem Topic vor \citep{davidmimnoOptimizingSemanticCoherence2011}. Zwischen den beiden Metriken besteht in der Regel ein negativer Zusammenhang. Daher ist es notwendig, eine Balance zwischen beiden zu finden.

  \begin{itemize}
  \tightlist
  \item
    Im vorliegenden Beispiel kÃ¶nnen wir zuerst die Modelle mit \(k \ge 80\) ausschlieÃŸen. Obwohl sie mehr Topics benÃ¶tigen, sind ihre Topics im Mittel weniger exklusiv und weniger kohÃ¤rent als die Topics der Modelle \(k = 60\) oder \(k = 70\). Eine ErklÃ¤rung dafÃ¼r kann sein, irgendwann zwischen dem 70. und dem 80. Topic keine substantiell neuen Aspekte mehr im Korpus zu finden sind. Die neuen Topics sind dann redundant zu schon bestehenden Topics.
  \item
    Ebenso kÃ¶nnen wir fÃ¼r die meisten Zwecke die Modelle mit \(k \le 20\) vernachlÃ¤ssigen, da die mittlere ExklusivitÃ¤t ihrer Topics wesentlich geringer ist als die der Ã¼brigen Modelle. Das Modell mit \(k = 20\) kÃ¶nnte vielleicht infrage kommen, wenn wir Sparsamkeit sehr hoch gewichten, also den Korpus durch mÃ¶glichst wenige Topics beschreiben wollen.
  \item
    SchlieÃŸlich ist das Modell mit \(k = 50\) nicht besonders attraktiv. Die Topics sind im Mittel nur weniger kohÃ¤rent, aber nicht exklusiver, als die Topics des Modells mit \(k = 40\). Der Detailvergleich der beiden Modelle in der nÃ¤chsten Abbildung, in der jedes Topic mit einem Punkt dargestellt ist, zeigt, dass dies vor allem auf vier Topics zurÃ¼ck geht, die weniger kohÃ¤rent sind, ohne eine besonders gute ExklusivitÃ¤t aufzuweisen.
  \item
    Gegeben der Metriken \emph{semantische KohÃ¤renz} und \emph{ExklusivitÃ¤t} spricht einiges dafÃ¼r, entweder Modelle mit ca. 60-70 Topics oder Modelle mit 30-40 Topics weiter zu verfolgen.
  \end{itemize}
\item
  Die \emph{held-out likelihood} und die \emph{Dispersion der multinomialen Residuen} sind zwei Metriken, die die Abweichung des Modells von den Daten quantifizieren. Auch sie sind wieder im relativen Modellvergleich zu interpretieren. Die held-out likelihood gibt Auskunft darÃ¼ber, wie gut das Vorkommen von WÃ¶rter in einem Dokument, das nicht zum SchÃ¤tzen des Modells genutzt wurde, vorhergesagt werden kann. Mit der Dispersion der multinomialen Residuen wird die Abweichung der durch das Modell vorhergesagten von den beobachteten WÃ¶rtern in den Dokumenten auf Basis des gesamten Datensatzes quantifiziert. Ein Wert von 1 wÃ¤re ideal, wird in angewandten Beispielen mit echten Texten aber kaum erreicht. Beide Metriken sind eng verwandt und legen in der Regel Ã¤hnliche Entscheidungen nahe.

  \begin{itemize}
  \tightlist
  \item
    Das Modell mit \(k = 50\) ist nach beiden Metriken gut geeignet.
  \item
    Nach der held-out likelihood sind die Modelle mit \(k \ge 80\) noch etwas besser - diese haben sich aber in der semantischer KohÃ¤renz und ExklusivitÃ¤t nicht sonderlich gut bewÃ¤hrt.
  \item
    Die nach semantischer KohÃ¤renz und ExklusivitÃ¤t besten Modelle liegen nach diesen beiden Metriken etwa gleich auf.
  \end{itemize}
\item
  Die Befunde sind auch fÃ¼r die didaktischen Zwecke dieses Workshops gut geeignet. Es wird klar, dass die Modellwahl sich nicht einfach automatisieren lÃ¤sst. Die quantitativen Kriterien helfen und lediglich, den Raum mÃ¶glicher Modelle einzuschrÃ¤nken.

  \begin{itemize}
  \tightlist
  \item
    FÃ¼r mich kommen auf Basis der berichteten Metriken Modelle mit zwischen 30 und 70 Topics infrage.
  \item
    Wenn man an einer besonders sparsamen LÃ¶sung interessiert ist, kÃ¶nnte man auch \(k = 20\) in ErwÃ¤gung ziehen.
  \item
    Wenn man besonders detailliertere LÃ¶sungen sucht, sind auch die Modelle mit \(k \ge 80\) nicht ausgeschlossen. Hier mÃ¼sste man dann die Differenzierungen zwischen den Topics in der Tiefe untersuchen und klarstellen.
  \end{itemize}
\item
  Im nÃ¤chsten Schritt vergleichen wir die Modelle mit 30 und mit 60 Topics, um zu verstehen, welche Konsequenzen es fÃ¼r die inhaltliche Interpretation hat, ein eher sparsames oder ein eher detailliertes Modell zu wÃ¤hlen. Nach dieser Entscheidung kÃ¶nnen wir dann weiter Ã¼berlegen, welches der eher sparsamen bzw. detaillierten Modelle fÃ¼r unser Forschungsinteresse besser geeignet ist.
\end{itemize}

\hypertarget{qualitativer-vergleich-der-modelle-mit-30-und-60-topics}{%
\subsection*{Qualitativer Vergleich der Modelle mit 30 und 60 Topics}\label{qualitativer-vergleich-der-modelle-mit-30-und-60-topics}}
\addcontentsline{toc}{subsection}{Qualitativer Vergleich der Modelle mit 30 und 60 Topics}

\begin{itemize}
\tightlist
\item
  Um die Ergebnisse eines Topic Model interpretieren zu kÃ¶nnen, mÃ¼ssen wir zuerst verstehen, wie das Ergebnis eines Topic Models aussieht. Es enthÃ¤lt im wesentlichen zwei zweidimensionale Vektoren von Koeffizienten.

  \begin{itemize}
  \tightlist
  \item
    beta: FÃ¼r jedes Topic die Wahrscheinlichkeit, dass ein Dokument, in dem das Feature vorkommt, das Topic hat. Jedes Topic erhÃ¤lt fÃ¼r jedes Feature einen beta-Koeffizienten zwischen 0 und 1, die zusammen 1 ergeben.
  \item
    theta bzw. gamma (uneinheitlich benannt): FÃ¼r jedes Dokument die Wahrscheinlichkeit, dass das Dokument das Topic enthÃ¤lt. Jedes Dokument erhÃ¤lt fÃ¼r jedes Topic einen theta- bzw. gamma-Koeffizienten zwischen 0 und 1, die zusammen 1 ergeben.
  \end{itemize}
\item
  Um die Bedeutung der Topics zu interpretieren, betrachten wir daher zwei Modell-Outputs.

  \begin{itemize}
  \tightlist
  \item
    Die Features, die am typischsten fÃ¼r Dokumente mit einem Topic sind, also die hÃ¶chsten beta-Koeffizienten (ggf. nach Korrekturen fÃ¼r die Verteilung der Features im gesamten Korpus).
  \item
    Die Dokumente, die mit der grÃ¶ÃŸten Wahrscheinlichkeit ein Thema enthalten (manchmal auch interpretiert als zum grÃ¶ÃŸten Anteil aus einem Thema bestehen), also die Dokumente mit den hÃ¶chsten gamma- bzw. theta-Koeffizienten.
  \end{itemize}
\item
  Zur qualitativen Interpretation der Modelle anhand dieser Outputs kann ich das Paket \texttt{\{stminsights\}} empfehlen --- vor allem denjenigen, die lieber mit einer intuitiven grafischen BenutzeroberflÃ¤che als direkt mit \emph{R} arbeiten. Aber auch ich selbst schÃ¤tze das Tool fÃ¼r einen schnellen Ãœberblick Ã¼ber mehrere Modelle.
\item
  Leider ist es zurzeit nicht einfach, \texttt{\{stminsights\}} direkt zum Laufen zu bringen, da einige Pakete, auf denen es aufbaut, Bugs bzw. KompabilitÃ¤tsprobleme haben. Eine Anleitung, wie das Paket zurzeit installiert werden kann, gibt es hier:
\end{itemize}

\begin{quote}
Important note: The shiny app for the CRAN release of stminsights does currently not work properly due to bugs introduced by recent changes in the Shiny package {[}\ldots{]}. Please use the Github version of stminsights for now. This will require the development version of Shiny which can be installed by running devtools::install\_github(`rstudio/shiny').
\end{quote}

\begin{quote}
You can download and install the latest development version of stminsights by running devtools::install\_github(`cschwem2er/stminsights') --- \url{https://github.com/cschwem2er/stminsights}
\end{quote}

\begin{itemize}
\tightlist
\item
  Zur Vorbereitung der Analyse mit \texttt{\{stminsights\}} mÃ¼ssen die Objekte der geschÃ¤tzten Modelle (hier: \texttt{m30} und \texttt{m70}) und die Daten, die zur ModellschÃ¤tzung verwendet wurden (hier: \texttt{out\ =\ Ã¬mpf\_stm}), als \texttt{.rdata} Datei gespeichert werden. Dabei muss das Daten-Objekt unbedingt den Namen \texttt{out} haben. Damit der Text der Dokumente in \texttt{\{stminsights\}} angezeigt werden kann, muss dieser zusÃ¤tzlich als Variable in den Meta-Daten enthalten sein (siehe Abschnitt zur Datenaufbereitung).
\item
  Durch das ausfÃ¼hren der Funktion \texttt{run\_stminsights()} wird eine grafische BenutzeroberflÃ¤che im Browser gestartet, mit dem die qualitative Analyse durchgefÃ¼hrt wird. Eine Beschreibung der OberflÃ¤che findet ihr als Video im LMS {[}kommt nach Fertigstellen des Textmaterials{]}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{run_stminsights}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Wer die Interpretation der Topics lieber in R durchfÃ¼hrt, kann z.B. den folgenden Code verwenden und anpassen.

  \begin{itemize}
  \tightlist
  \item
    Wir sammeln zuerst die 20 typischsten Texte fÃ¼r jedes Topic in einem Datensatz. Dabei bereiten wir die Texte auch fÃ¼r eine Ausgabe in der Konsole vor. Der Parameter ``gamma'' oder ``theta'' (uneinheitlich benannt, aber derselbe Parameter) kann mit \texttt{tidytext::tidy()} extrahiert werden. Dann werden die Texte aus den Ursprungsdaten zugespielt.
  \item
    Mit \texttt{stm::labelTopics()} kÃ¶nnen wird die typischsten Features fÃ¼r ein Topic anzeigen. \texttt{n} bestimmt die Zahl der Features, \texttt{topics} das Topic, das wir gerade beschreiben mÃ¶chten. Es werden die typischsten Features nach vier verschiedenen Kriterien ausgegeben (Diese werden auch in \texttt{\{stminsights\}} angezeigt):

    \begin{itemize}
    \tightlist
    \item
      \emph{Highest Prob} zeigt die Features, die mit der grÃ¶ÃŸten Wahrscheinlichkeit in Dokumenten mit einem Topic vorkommen (= Features mit den hÃ¶chsten beta-Koeffizienten fÃ¼r das Topic). Dabei wird die GesamthÃ¤ufigkeit der Features im Korpus \emph{nicht} berÃ¼cksichtigt. WÃ¶rter, die allgemein sehr hÃ¤ufig vorkommen, sind nach dieser Metrik typisch fÃ¼r verschiedene Topics. Die anderen drei Metriken versuchen, diese SchwÃ¤che auf verschiedene Art und Weise zu korrigieren.
    \item
      \emph{FREX} steht fÃ¼r \emph{most frequent and exclusive} Features. Hier werden die Features aufgelistet, die mÃ¶glichst typisch fÃ¼r ein Dokument mit einem Topic, aber mÃ¶glichst nicht sehr typisch fÃ¼r Dokumente mit anderen Topics sind. Siehe \texttt{?calcfrex} fÃ¼r technische Details.
    \item
      Die \emph{Lift}-Metrik setzt die Wahrscheinlichkeit, dass ein Feature in einem Dokument mit einem Topic vorkommt, zur Wahrscheinlichkeit, dass ein Feature in einem beliebigen Dokument vorkommt, ins VerhÃ¤ltnis. Siehe \texttt{?calclift} fÃ¼r technische Details.
    \item
      \emph{Score} gewichtet fÃ¼r die Wahrscheinlichkeit, mit der ein Feature in Dokumenten mit einem anderen Topics vorkommt. Siehe \texttt{?calcscore} fÃ¼r technische Details.
    \end{itemize}
  \item
    Mit den AuszÃ¼gen aus dem Datensatz typischer Dokumente (gefiltert nach Topic) kÃ¶nnen wir diese Features zudem im Kontext der gesamten Texte sehen.
  \item
    Im abschlieÃŸenden Datensatz \texttt{topic\_labels} halten wir fÃ¼r jedes Topic ein aussagekrÃ¤ftiges, mÃ¶glichst kurzes Label fest, das wir jetzt zur eigenen Ãœbersicht und spÃ¤ter auch zur Ergebnisdarstellung verwenden. ZusÃ¤tzlich empfehle ich, eine kurze Zusammenfassung fÃ¼r jedes Topic auf einem Medium eigener Wahl (prÃ¤ferierte analoger oder digitaler Notizzettel) festzuhalten.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Laden der Modelle}
\KeywordTok{load}\NormalTok{(}\StringTok{"R/data/models30_60.rdata"}\NormalTok{)}

\CommentTok{# Beispiel fÃ¼r das Modell mit k = 30}
\CommentTok{# Erstellen eines Datensatzes mit den typischsten Dokumenten}
\NormalTok{top_docs =}\StringTok{ }\KeywordTok{tidy}\NormalTok{(m30, }\StringTok{"gamma"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(gamma)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(topic) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{rank =} \DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{left_join}\NormalTok{(}\KeywordTok{mutate}\NormalTok{(}\KeywordTok{select}\NormalTok{(out}\OperatorTok{$}\NormalTok{meta, txt), }\DataTypeTok{document =} \DecValTok{1}\OperatorTok{:}\KeywordTok{n}\NormalTok{())) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{out =} \KeywordTok{paste0}\NormalTok{(}\KeywordTok{round}\NormalTok{(gamma, }\DecValTok{2}\NormalTok{), }\StringTok{": "}\NormalTok{, txt))}

\CommentTok{# Ausgabe der typischen Feature fÃ¼r ein Topic (hier Topic 1)}
\NormalTok{m30 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{labelTopics}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10}\NormalTok{, }\DataTypeTok{topics =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Topic 1 Top Words:
##       Highest Prob: 2, 3, 1, monate, erst, monaten, 4, 6, alt, 5 
##       FREX: 3, 2, monate, monaten, 6, 4, 1, monat, +, 5 
##       Lift: Ã¶sterreich, zecken, monat, fach, 3, 2, monate, 6, +, monaten 
##       Score: Ã¶sterreich, 2, 3, monate, 1, monaten, 4, alt, 6, +
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Ausgabe der typischen Texte fÃ¼r ein Topic (hier Topic 1)}
\NormalTok{top_docs }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(topic }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(rank }\OperatorTok{%in%}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{.}\OperatorTok{$}\NormalTok{out }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{str_squish}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{cat}\NormalTok{(}\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 0.62: da fÃ¤ngt der 12 lebensmonat an, ein Jahr alt ist dein Baby dann erst mit vollendetem 12. Lebensmonat.... meine Tochter wird jetzt am 24.07 12 Monate alt, und am 24.08 wird sie 1 Jahr....da ist also der 12 lebensmonat vollendet :)
## 
## 0.59: Man spricht erst sobald der Monat vollendet ist vom z.B. 12. Monat. Deine Tochter ist also einen Monat vor ihrem 1. Geburtstag ELF Monate alt und nicht 12... Klar, sie befindet sich ab dem 24.07. im zwÃ¶lften Lebensmonat, ist aber noch keine 12 Monate alt, sondern 11. Kann etwas verwirrend sein, ich weiÃŸ... Denn sie ist am Tag ihrer Geburt ja auch nicht einen Monat alt... Sondern erst einen Monat nach ihrer Geburt Ich bin aktuell 30 Jahre alt und befinde mich somit in meinem 31. Lebensjahr. So wird das immer gerechnet :)
## 
## 0.59: In Ã–sterreich wird im 3., 5. und 12. Monat geimpft. Ich bin auf einer anderen Seite auch fÃ¼ndig geworden. Dieses Schema nennt sich 2+1. Das in Deutschland von der Stiko empfohlene Schema nennt sich 3+1. Beide Schemata werden wohl in Deutschland als Grundimmunisierung anerkannt, da das Schema 2+1 einen Ã¤hnlichen bzw. gleichen Effekt hat wie 3+1. Die Stiko empfiehlt halt das 3+1 Schema.
## 
## 0.53: so mein ich ja :) sie befindet sich ab dem Tag ihrer Geburt ja im erst Lebensmonat und nicht im nullsten Lebensmonat :D und ab dem 24.07 ist sie im 12. Ã–ebensmonat.... wenn mich jemand FrÃ¤gt wie alt sie ist sag ich auch nich 12 Monate alt sonder sie wird Ende August 1 Jahr alt...aber prinzipiell, so wird es auch bei Urbia angezeigt ist sie ab 24.07 im 12 lebensmonat....und wenn der vollendet ist, wird sie 1... :)
## 
## 0.51: Wir mussten 2 Monate auf den Termin beim Kardiologen warten. Hatten den Termin dann als er jetzt 6 Monate alt war. Das HerzgerÃ¤usch sind 2 SehnenfÃ¤den, aber er hat auch ein Mini Loch. Wir mÃ¼ssen erst wieder zur Kontrolle, wenn er 3 Jahre ist.
## 
## 0.5: Korrektur: Bei der 2+1 Impfung sind ca 84% nach der 2 . Impfung immun. Bei der 3+1 Impfung sind ca 95 % nach der 3. Impfung immun. Und die ersten beiden bzw. 3 Impfungen werden ja in einen Zeitraum von 2 Monaten verabreicht. Also durchaus noch abzuwarten, wenn du dir da Sorgen machst.
## 
## 0.47: hallo rebecca es gibt auch "Mittelwege", zB das 2+1-Schema. Man beginnt erst mit 3 Monaten und spart sich eine Impfdosis. (Normalerweise ist ja 3 x innerhalb des 1. Lebensjahres und 1 x zur Auffrischung). Die Immunantwort ist am Ende wohl identisch, das Risiko besteht darin, dass man eben 1 Monat spÃ¤ter beginnt und damit der Impfschutz 1 monat spÃ¤ter beginnt. Wobei das grÃ¶ÃŸte Risiko hier der Keuchhusten ist. Das 2+1-Schema ist zB in Ã–sterreich Standard.
## 
## 0.47: Unter 2 Jahren muss 2x geimpft werden. Mein Sohn ist 6 Monate. Wird am Freitag geimpft und dann in 2 Monaten wieder. Die Dritte erfolgt wenn er 2 Jahre alt ist oder 3 Jahre. Du musst alle 3 Impfungen bezahlen jeweils 120 Euro oder ein bisschen mehr. Die AOK Niedersachsen wird diese 3 Impfungen jeweils mit 80% mir erstatten. Die AOK Sachsen anscheinend alle 3 zu 100%.
## 
## 0.44: Dann wÃ¤r er aber 1 Monat vor Geburtstag 12 Monate : Wenn du die Wochen zÃ¤hlst darfst du halt nicht 4 Wochen mit 1 Monat gleich setzen.
## 
## 0.43: Darf ich Ã¼ber die Fragestellung hinaus fragen, wie genau der Ablauf dann genau ist. Befasse mich in den ersten ZÃ¼gen mit dem Thema Impfen. Es wird also in den ersten 12 Monaten 2x 5fach geimpft und dann 1x bis zum 24 Monat? Kann man das system 2+1 auch auf 6fach-impfung Ã¼bertragen? Und auf +pneumokokken?
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Datensatz fÃ¼r Topic Labels}
\CommentTok{# Bis Topic 30 weiterfÃ¼hren}
\NormalTok{topic_labels =}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \OperatorTok{~}\NormalTok{topic, }\OperatorTok{~}\NormalTok{label,}
  \DecValTok{1}\NormalTok{,      }\StringTok{"Alter von Babies"}\NormalTok{,}
  \DecValTok{2}\NormalTok{,      }\StringTok{"label B"}\NormalTok{,}
  \DecValTok{3}\NormalTok{,      }\StringTok{"label C"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \bibliography{book.bib}

\end{document}
