# Modellinterpretation und -darstellung

```{r, include=FALSE}
source("R/packages.R")
source("R/read_data.R") # am Ende entfernen, nur für temp knit
source("R/prepare_data.R")
```

```{r, echo=FALSE, cache=FALSE}
knitr::read_chunk("R/oolong_intro.R")
knitr::read_chunk("R/prevalence_plot.R")
knitr::read_chunk("R/topic_cluster.R")
knitr::read_chunk("R/covariates.R")
```

## Test der Modellqualität

* `{oolong}` ist ein recht neues Paket, das die Durchführung von standardisierten Tests der Topic-Qualität erleichtert. Ich habe es selbst bisher noch nicht in produktiven Projekten eingesetzt, werde es aber in allen zukünftigen Projekten tun. Infos zur technischen Umsetzung gibt es hier: https://github.com/chainsawriot/oolong/blob/master/overview_gh.md.

* Es werden zwei Tests angeboten:
  * Der *word intrusion test* zeigt für jedes Topic die typischsten Features an und fügt zusätzlich ein Wort hinzu, dass *nicht* typisch für das Topic ist (*intruder*). Die Tester\*innen müssen raten, welches Wort nicht zu den anderen passt. Je größer der Anteil der richtig erkannten *intruder* (Hier als *precision* bezeichnet), desto besser lassen sich die Topics anhand der typischen Features interpretieren. Bei mehreren Tester\*innen wird zusätzlich der aus der manuellen Inhaltsanalyse bekannte Koeffizient der Intercoder-Reliabilität Krippendorffs $\alpha$ ausgegeben.
  * Der *topic intrusion test* zeigt eine Auswahl von Dokumenten aus dem Korpus. Dazu werden eine vorgegebene Zahl von Topics angezeigt, die in einem Dokument am wahrscheinlichsten enthalten sind. Ein weiteres Topic wird angezeigt, das in diesem Dokument nicht enthalten ist. Die Tester\*innen müssen raten, welches Topic nicht zum Dokument passt. Als Ergebnis werden die *topic log odds* (TLO) berichtet. Sie quantifizieren die Wahrscheinlichkeit, dass das unpassende Topic gewählt wurde, korrigiert um die Wahrscheinlichkeit, dass das unpassende Topic einfach nur zufällig geraten wurde. Perfektes Erkennen der falschen Topics bei allen Dokumenten ergibt $TLO = 0$.
  * Diese Tests sollten von mehreren Personen durchgeführt werden. Im Idealfall werden die Modelle auch durch Personen getestet, die nicht am Projekt (oder zumindest nicht an der Modellierung) beteiligt waren, um die intersubjektive Nachvollziehbarkeit der Interpretationen zu testen.
  * Aber schon formale Tests nur mit den Projektbeteiligten schlagen den heute üblichen Standard um Weiten. Bisher werden zur Validierung vor allem informelle Diskussionen eingesetzt [@maierApplyingLDATopic2018] --- so auch in unserer Beispielstudie.

* Der folgende Code zeigt das Erstellen, durchführen und Auswerten der Tests. Da die Tests interaktiv sind, wird er hier nicht ausgeführt. Ich zeige das Vorgehen in einem Video im LMS [kommt nach Fertigstellen des Textmaterials].

```{r oolong-intro, eval=FALSE}

```


## Darstellung des Modells

* Einige nützliche Plots

```{r prevalence-plot, fig.width=16, fig.cap="Zur vollen Ansicht in neuem Tab öffnen", message=FALSE, warning=FALSE}

```

* Tabelle mit FREX-Termen

* Autoreport (Verweis auf Dokumentation in Bachl & Link)


## Weitere Analysen: Prädiktoren der Topic-Prävalenz und Topic-Cluster

* Schätzen der Veränderung der Topics über die Zeit
* Wichtig: Prädiktoren sollten bereits in der Spezifikation des Topic Modells (siehe Abschnitt 3.1) enthalten sein. Schätzung der Zusammenhänge mit anderen Prädiktoren zwar möglich, aber nicht ideal [@robertsStmPackageStructural2019].
* Darstellung dieser Ergebnisse

```{r covariates, message=FALSE, warning=FALSE}

```

```{r, fig.height=16, fig.cap="Zur vollen Ansicht in neuem Tab öffnen", echo=FALSE}
plt1
```

```{r, fig.cap="Zur vollen Ansicht in neuem Tab öffnen", echo=FALSE}
plt2
```


* Cluster-Analyse des gemeinsamen Auftretens von Topics
* Darstellung dieser Ergebnisse

```{r topic-cluster, fig.width=16, fig.cap="Zur vollen Ansicht in neuem Tab öffnen", message=FALSE, warning=FALSE}

```

